"""
Unified pipeline for processing documents - AI/ML Course Assistant

This pipeline orchestrates all stages of document processing:
1. Extract: Images and text from PDFs
2. Caption: Generate enriched captions for images
3. Chunk: Split text into semantic chunks
4. Embed: Generate embeddings for chunks and images
5. Index: Store in ChromaDB

Features:
- Incremental processing (skip already processed docs)
- Resume after interruption
- Force reprocessing if needed
- Registry tracking with status and costs

Usage:
    # Process all documents (skip completed)
    python run_pipeline.py process --all
    
    # Process specific document
    python run_pipeline.py process --doc-id arxiv_1706_03762
    
    # Force reprocess (ignore registry)
    python run_pipeline.py process --doc-id arxiv_1706_03762 --force
    
    # Process only new documents
    python run_pipeline.py process --new-only
    
    # Show processing status
    python run_pipeline.py status
"""

import json
import logging
import argparse
import os
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

from ingest.extract_images_smart import extract_document as extract_pdf_document
from ingest.extract_from_json import extract_json_document
from ingest.enrich_images import generate_captions_for_doc
from index.chunk_documents import chunk_document_with_image_tracking
from index.embedding_utils import embed_document
from index.build_index import index_documents_to_chromadb

# Load environment variables
load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)

PROJECT_ROOT = Path(__file__).parent


def validate_environment():
    """
    Validate required environment variables before pipeline execution.
    
    Raises:
        EnvironmentError: If required variables are missing with actionable guidance
    """
    if not os.getenv("OPENAI_API_KEY"):
        raise EnvironmentError(
            "\n‚ùå OPENAI_API_KEY not found.\n\n"
            "Required for embeddings and VLM captions.\n"
            "Please create a .env file in the project root with:\n\n"
            "  OPENAI_API_KEY=your_key_here\n\n"
            "Or set the environment variable:\n"
            "  Windows: set OPENAI_API_KEY=your_key_here\n"
            "  Linux/Mac: export OPENAI_API_KEY=your_key_here\n"
        )
    logging.info("‚úÖ Environment validation passed")

REGISTRY_PATH = PROJECT_ROOT / "data" / "processed_docs.json"
RAW_PAPERS_DIR = PROJECT_ROOT / "data" / "raw" / "papers"
IMAGES_OUTPUT_DIR = PROJECT_ROOT / "data" / "processed" / "images"


def load_registry(registry_path: Path = REGISTRY_PATH) -> Dict:
    """Load processing registry from JSON file."""
    if registry_path.exists():
        with open(registry_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_registry(registry: Dict, registry_path: Path = REGISTRY_PATH):
    """Save processing registry to JSON file."""
    registry_path.parent.mkdir(parents=True, exist_ok=True)
    with open(registry_path, 'w', encoding='utf-8') as f:
        json.dump(registry, f, indent=2, ensure_ascii=False)
    logging.debug(f"Registry saved: {len(registry)} documents")


def update_registry(doc_id: str, updates: Dict, registry_path: Path = REGISTRY_PATH):
    """
    Update registry for a specific document.
    
    Args:
        doc_id: Document identifier
        updates: Dictionary with updates (supports nested keys like "stages.extract")
        registry_path: Path to registry file
    
    Example:
        update_registry("arxiv_1706_03762", {
            "status": "in_progress",
            "stages.extract": "completed",
            "stats.images_count": 8
        })
    """
    registry = load_registry(registry_path)
    
    if doc_id not in registry:
        registry[doc_id] = {
            "doc_id": doc_id,
            "status": "pending",
            "stages": {},
            "stats": {},
            "cost": {}
        }

    for key, value in updates.items():
        if '.' in key:  # Nested key like "stages.extract"
            parts = key.split('.')
            target = registry[doc_id]
            for part in parts[:-1]:
                if part not in target:
                    target[part] = {}
                target = target[part]
            target[parts[-1]] = value
        else:
            registry[doc_id][key] = value
    
    save_registry(registry, registry_path)


def is_stage_completed(doc_id: str, stage: str, registry_path: Path = REGISTRY_PATH) -> bool:
    """Check if a processing stage is completed for a document."""
    registry = load_registry(registry_path)
    if doc_id not in registry:
        return False
    return registry[doc_id].get("stages", {}).get(stage) == "completed"


def is_document_completed(doc_id: str, registry_path: Path = REGISTRY_PATH) -> bool:
    """Check if document is fully processed."""
    registry = load_registry(registry_path)
    if doc_id not in registry:
        return False
    return registry[doc_id].get("status") == "completed"



def detect_document_type(doc_id: str) -> str:
    """Detect document type based on doc_id prefix."""
    if doc_id.startswith("arxiv_"):
        return "pdf"
    elif doc_id.startswith("realpython_") or doc_id.startswith("medium_"):
        return "json"
    else:
        raise ValueError(f"Unknown document type for doc_id: {doc_id}")


def process_document(
    doc_id: str,
    force: bool = False,
    use_vlm: bool = True
) -> Dict:
    """
    Process a single document through all stages.
    
    Args:
        doc_id: Document identifier (PDF filename without extension)
        force: If True, reprocess even if already completed
        use_vlm: If True, use Vision-LM for image captions (costs API calls)
    
    Returns:
        Processing result with stats and costs
    """
    logging.info(f"{'='*70}")
    logging.info(f"Processing document: {doc_id}")
    logging.info(f"{'='*70}")

    if is_document_completed(doc_id) and not force:
        logging.info(f"‚è≠Ô∏è  Skipping - already processed")
        registry = load_registry()
        return registry[doc_id]
    
    if force:
        logging.info(f"üîÑ Force reprocessing enabled")

    update_registry(doc_id, {
        "status": "in_progress",
        "started_at": datetime.now().isoformat()
    })
    
    result = {
        "doc_id": doc_id,
        "stages_completed": [],
        "errors": []
    }
    
    try:
        doc_type = detect_document_type(doc_id)
        logging.info(f"üìã Document type: {doc_type.upper()}")
        
        # ====================================================================
        # Stage 1: Extract images and text (PDF or JSON)
        # ====================================================================
        if not is_stage_completed(doc_id, "extract") or force:
            logging.info(f"\nüìÑ Stage 1: Extracting images and text...")
            
            if doc_type == "pdf":
                extract_result = extract_pdf_document(
                    doc_id=doc_id,
                    input_dir=RAW_PAPERS_DIR,
                    output_dir=IMAGES_OUTPUT_DIR
                )
            elif doc_type == "json":
                extract_result = extract_json_document(
                    doc_id=doc_id,
                    output_dir=IMAGES_OUTPUT_DIR
                )
            else:
                raise ValueError(f"Unsupported document type: {doc_type}")
            
            if "error" in extract_result:
                raise Exception(f"Extraction failed: {extract_result['error']}")

            # Store extract_result in memory for Stage 3 (NOT in registry - too large)
            result["extract_result"] = extract_result
            
            # Update registry with stats only
            update_registry(doc_id, {
                "stages.extract": "completed",
                "stats.images_count": extract_result["images_count"],
                "stats.text_length": extract_result["text_length"]
            })
            
            result["stages_completed"].append("extract")
            logging.info(f"‚úÖ Extract: {extract_result['images_count']} images, "
                        f"{extract_result['text_length']} chars")
        else:
            logging.info(f"‚è≠Ô∏è  Stage 1: Extract - already completed")
            # Stage 1 completed - need full pipeline restart with --force
            logging.warning(f"‚ö†Ô∏è  extract_result not available (stage completed). "
                          f"Use --force for full pipeline restart.")
        
      
        if not is_stage_completed(doc_id, "caption") or force:
            logging.info(f"\nüé® Stage 2: Generating enriched captions...")

            registry = load_registry()
            images_count = registry[doc_id].get("stats", {}).get("images_count", 0)
            
            if images_count > 0:
                captioned_count = generate_captions_for_doc(
                    doc_id=doc_id,
                    use_vlm=use_vlm
                )
                
                # Estimate cost (OpenAI GPT-4.1-mini Vision: ~$0.015 per image)
                caption_cost = captioned_count * 0.015 if use_vlm else 0.0
                
                update_registry(doc_id, {
                    "stages.caption": "completed",
                    "cost.captions": round(caption_cost, 3)
                })
                
                result["stages_completed"].append("caption")
                logging.info(f"‚úÖ Caption: {captioned_count} images enriched "
                           f"(cost: ${caption_cost:.3f})")
            else:
                logging.info(f"‚è≠Ô∏è  No images to caption")
                update_registry(doc_id, {"stages.caption": "completed"})
        else:
            logging.info(f"‚è≠Ô∏è  Stage 2: Caption - already completed")
        
        # ====================================================================
        # Stage 3: Chunk text (IN-MEMORY)
        # ====================================================================
        if not is_stage_completed(doc_id, "chunk") or force:
            logging.info(f"\nüìù Stage 3: Chunking text...")
            
            # Use extract_result from Stage 1 (in-memory only, not in registry)
            if "extract_result" not in result:
                logging.error(f"‚ùå No extract_result in memory for {doc_id}")
                logging.error(f"   Stage 1 must complete in same pipeline run")
                logging.error(f"   Use --force to restart full pipeline")
                return result
            
            extract_result = result["extract_result"]
            
            # Load images metadata
            images_metadata_path = PROJECT_ROOT / "data" / "processed" / "images_metadata.json"
            if images_metadata_path.exists():
                with open(images_metadata_path, 'r', encoding='utf-8') as f:
                    all_images = json.load(f)
                doc_images = [img for img in all_images if img['doc_id'] == doc_id]
            else:
                doc_images = []
            
            # Determine total_pages (0 for JSON documents)
            total_pages = extract_result.get("document_metadata", {}).get("num_pages", 0)
            
            # Chunk document
            chunks = chunk_document_with_image_tracking(
                doc_id=doc_id,
                full_text=extract_result["full_text"],
                total_pages=total_pages,
                images_metadata=doc_images,
                chunk_size=1700,  # ~500 tokens
                chunk_overlap=150  # ~60 tokens
            )
            
            # Store chunks in result for next stages (in-memory only)
            result["chunks"] = chunks
            
            # Update registry with stats only (chunks go to ChromaDB)
            update_registry(doc_id, {
                "stages.chunk": "completed",
                "stats.chunks_count": len(chunks)
            })
            
            # Statistics
            with_refs = sum(1 for c in chunks if c['has_figure_references'])
            with_related = sum(1 for c in chunks if len(c['related_image_ids']) > 0)
            
            result["stages_completed"].append("chunk")
            logging.info(f"‚úÖ Chunk: {len(chunks)} chunks created")
            logging.info(f"   - {with_refs} with figure references")
            logging.info(f"   - {with_related} with related images")
        else:
            logging.info(f"‚è≠Ô∏è  Stage 3: Chunk - already completed")
            # If chunk stage already completed, need to re-chunk or use --force
            logging.warning(f"‚ö†Ô∏è  Chunks not available (stage already completed). "
                          f"Use --force to regenerate chunks for embedding.")
            return result
        
        # ====================================================================
        # Stage 4: Generate embeddings (IN-MEMORY)
        # ====================================================================
        if not is_stage_completed(doc_id, "embed") or force:
            logging.info(f"\nüî¢ Stage 4: Generating embeddings...")
            
            # Initialize OpenAI client
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not set in environment")
            client = OpenAI(api_key=api_key)
            
            # Load images metadata for this document
            images_metadata_path = PROJECT_ROOT / "data" / "processed" / "images_metadata.json"
            if images_metadata_path.exists():
                with open(images_metadata_path, 'r', encoding='utf-8') as f:
                    all_images = json.load(f)
                doc_images = [img for img in all_images if img['doc_id'] == doc_id]
            else:
                doc_images = []
            
            # Generate embeddings
            embed_result = embed_document(
                doc_id=doc_id,
                chunks=chunks,
                images=doc_images,
                client=client
            )
            
            # Store embeddings in result for Stage 5
            result["chunks_with_embeddings"] = embed_result["chunks_with_embeddings"]
            result["images_with_embeddings"] = embed_result["images_with_embeddings"]
            
            # Update registry
            update_registry(doc_id, {
                "stages.embed": "completed",
                "cost.embeddings": embed_result["cost"]
            })
            
            result["stages_completed"].append("embed")
            logging.info(f"‚úÖ Embed: {embed_result['stats']['chunks_embedded']} chunks, "
                        f"{embed_result['stats']['images_embedded']} images")
        else:
            logging.info(f"‚è≠Ô∏è  Stage 4: Embed - already completed")
            # If already completed, we need to load embeddings for Stage 5
            # For now, we'll skip and require re-running with --force
            logging.warning(f"‚ö†Ô∏è  Embeddings not loaded (stage already completed). "
                          f"Use --force to regenerate.")
        
        # ====================================================================
        # Stage 5: Index to ChromaDB
        # ====================================================================
        if not is_stage_completed(doc_id, "index") or force:
            logging.info(f"\nüíæ Stage 5: Indexing to ChromaDB...")
            
            # Check if we have embeddings in memory
            if "chunks_with_embeddings" not in result or "images_with_embeddings" not in result:
                raise ValueError(
                    f"Cannot index {doc_id}: embeddings not found in memory. "
                    f"Use --force to regenerate embeddings."
                )
            
            # Index to ChromaDB (with locking and native API)
            index_stats = index_documents_to_chromadb(
                doc_id,
                result["chunks_with_embeddings"],
                result["images_with_embeddings"]
            )
            
            logging.info(f"‚úÖ Index: {index_stats['text_chunks_added']} chunks, "
                        f"{index_stats['images_added']} images added")
            
            update_registry(doc_id, {
                "stages.index": "completed",
                "stats.indexed_chunks": index_stats['text_chunks_added'] + index_stats['text_chunks_skipped'],
                "stats.indexed_images": index_stats['images_added'] + index_stats['images_skipped']
            })
            
            result["stages_completed"].append("index")
        else:
            logging.info(f"‚è≠Ô∏è  Stage 5: Index - already completed")
        
        # ====================================================================
        # Mark as completed
        # ====================================================================
        registry = load_registry()
        total_cost = sum(registry[doc_id].get("cost", {}).values())
        
        update_registry(doc_id, {
            "status": "completed",
            "completed_at": datetime.now().isoformat(),
            "cost.total": round(total_cost, 3)
        })
        
        logging.info(f"\n{'='*70}")
        logging.info(f"‚úÖ Document processing completed!")
        logging.info(f"Total cost: ${total_cost:.3f}")
        logging.info(f"{'='*70}")
        
        return registry[doc_id]
        
    except Exception as e:
        logging.error(f"\n‚ùå Error processing {doc_id}: {e}")
        update_registry(doc_id, {
            "status": "failed",
            "error": str(e),
            "failed_at": datetime.now().isoformat()
        })
        result["errors"].append(str(e))
        raise


def process_all_documents(
    force: bool = False,
    new_only: bool = False,
    use_vlm: bool = True
):
    """
    Process all documents in data/raw/papers/.
    
    Args:
        force: Reprocess all documents
        new_only: Only process documents not in registry
        use_vlm: Use Vision-LM for captions
    """
    # Find all PDFs
    pdf_files = list(RAW_PAPERS_DIR.glob("*.pdf"))
    
    if not pdf_files:
        logging.error(f"No PDF files found in {RAW_PAPERS_DIR}")
        return
    
    doc_ids = [pdf.stem for pdf in pdf_files]
    
    # Filter based on registry
    registry = load_registry()
    
    if new_only:
        doc_ids = [doc_id for doc_id in doc_ids if doc_id not in registry]
        logging.info(f"Processing {len(doc_ids)} new documents (not in registry)")
    else:
        logging.info(f"Processing {len(doc_ids)} documents total")
    
    if not doc_ids:
        logging.info("No documents to process")
        return
    
    # Process each document
    success_count = 0
    failed_count = 0
    skipped_count = 0
    
    for idx, doc_id in enumerate(doc_ids, 1):
        logging.info(f"\n{'='*70}")
        logging.info(f"Document [{idx}/{len(doc_ids)}]: {doc_id}")
        logging.info(f"{'='*70}")
        
        try:
            if is_document_completed(doc_id) and not force:
                logging.info(f"‚è≠Ô∏è  Already completed - skipping")
                skipped_count += 1
                continue
            
            process_document(doc_id, force=force, use_vlm=use_vlm)
            success_count += 1
            
        except Exception as e:
            logging.error(f"‚ùå Failed: {e}")
            failed_count += 1
            continue
    
    # Summary
    logging.info(f"\n{'='*70}")
    logging.info(f"üìä Processing Summary")
    logging.info(f"{'='*70}")
    logging.info(f"Total documents: {len(doc_ids)}")
    logging.info(f"‚úÖ Successful: {success_count}")
    logging.info(f"‚è≠Ô∏è  Skipped: {skipped_count}")
    logging.info(f"‚ùå Failed: {failed_count}")
    logging.info(f"{'='*70}")


def show_status():
    """Show processing status for all documents."""
    registry = load_registry()
    
    if not registry:
        logging.info("Registry is empty - no documents processed yet")
        return
    
    logging.info(f"\n{'='*70}")
    logging.info(f"üìä Processing Status")
    logging.info(f"{'='*70}\n")
    
    completed = [d for d in registry.values() if d.get("status") == "completed"]
    in_progress = [d for d in registry.values() if d.get("status") == "in_progress"]
    failed = [d for d in registry.values() if d.get("status") == "failed"]
    
    logging.info(f"Total documents in registry: {len(registry)}")
    logging.info(f"  ‚úÖ Completed: {len(completed)}")
    logging.info(f"  üîÑ In Progress: {len(in_progress)}")
    logging.info(f"  ‚ùå Failed: {len(failed)}")
    
    # Calculate total costs
    total_cost = sum(d.get("cost", {}).get("total", 0) for d in completed)
    logging.info(f"\nüí∞ Total cost: ${total_cost:.3f}")
    
    # Show in-progress documents
    if in_progress:
        logging.info(f"\nüîÑ In Progress:")
        for doc in in_progress:
            stages = doc.get("stages", {})
            completed_stages = [s for s, status in stages.items() if status == "completed"]
            logging.info(f"  - {doc['doc_id']}: {len(completed_stages)}/5 stages")
    
    # Show failed documents
    if failed:
        logging.info(f"\n‚ùå Failed:")
        for doc in failed:
            error = doc.get("error", "Unknown error")
            logging.info(f"  - {doc['doc_id']}: {error}")


# ============================================================================
# CLI
# ============================================================================

def main():
    # Validate environment before any processing
    try:
        validate_environment()
    except EnvironmentError as e:
        logging.error(str(e))
        return 1
    
    parser = argparse.ArgumentParser(
        description="Unified document processing pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process all documents
  python run_pipeline.py process --all
  
  # Process specific document
  python run_pipeline.py process --doc-id arxiv_1706_03762
  
  # Force reprocess
  python run_pipeline.py process --doc-id arxiv_1706_03762 --force
  
  # Process only new documents
  python run_pipeline.py process --new-only
  
  # Show status
  python run_pipeline.py status
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="Command to execute")
    
    # Process command
    process_parser = subparsers.add_parser("process", help="Process documents")
    process_parser.add_argument("--doc-id", type=str, nargs='+', help="Process specific document(s)")
    process_parser.add_argument("--all", action="store_true", help="Process all documents")
    process_parser.add_argument("--new-only", action="store_true", help="Process only new documents")
    process_parser.add_argument("--force", action="store_true", help="Force reprocess")
    process_parser.add_argument("--no-vlm", action="store_true", help="Skip Vision-LM captions (save costs)")
    
    # Status command
    status_parser = subparsers.add_parser("status", help="Show processing status")
    
    args = parser.parse_args()
    
    if args.command == "process":
        if args.doc_id:
            # Process single or multiple documents
            for doc_id in args.doc_id:
                process_document(doc_id, force=args.force, use_vlm=not args.no_vlm)
        elif args.all or args.new_only:
            # Process multiple documents
            process_all_documents(force=args.force, new_only=args.new_only, use_vlm=not args.no_vlm)
        else:
            logging.error("Specify --doc-id, --all, or --new-only")
            return
    
    elif args.command == "status":
        show_status()
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
