# Retrieval Evaluation Analysis & Improvements
**Date:** 2026-01-08  
**Evaluation:** Quick Start (10 queries)  
**Results File:** [retrieval_eval_20260108_134956.json](results/retrieval_eval_20260108_134956.json)

---

## ğŸ“Š Current Results

### âœ… **Text Retrieval (EXCELLENT)**
- **Recall@5: 95.0%** (target â‰¥70%) âœ… **+25% above target**
- **MRR: 1.000** (target â‰¥0.70) âœ… **Perfect score**
- **Precision@5: 24%** - acceptable for RAG (diversity over precision)

**Analysis:** Text retrieval Ğ¿Ñ€Ğ°Ñ†ÑÑ” Ğ²Ñ–Ğ´Ğ¼Ñ–Ğ½Ğ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ– Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸ Ğ· Ğ¿ĞµÑ€ÑˆĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñƒ (MRR=1.0) Ñ– Ğ¿Ğ¾ĞºÑ€Ğ¸Ğ²Ğ°Ñ” Ğ¼Ğ°Ğ¹Ğ¶Ğµ Ğ²ÑÑ– Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ– docs.

---

### âŒ **Image Retrieval (NEEDS IMPROVEMENT)**
- **Image Hit Rate: 55.6%** (target â‰¥60%) âŒ **-4.4% below target**
- **Image Recall: 0.0%** âŒ **ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°**

**Breakdown:**
- Visual queries: 9 total
- Queries with images: 5 (55.6%)
- Queries without images: 4 (44.4%)

**Failed queries:**
1. Query 1: "Explain dropout regularization" - 0 images (expected 2)
2. Query 5: "Explain gradient descent" - 0 images (expected 2)
3. Query 6: "How to use NumPy arrays?" - 0 images (expected 2)
4. Query 8: "How do AI agents plan tasks?" - 0 images (expected 1)

---

## ğŸ” Root Cause Analysis

### **Problem: Image Recall = 0.0%**

ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ğ² Ğ»Ğ¾Ğ³Ğ¸ evaluation Ñ– Ğ·Ğ½Ğ°Ğ¹ÑˆĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ:

```
13:49:37 - INFO - Fetched 0 images by ID
13:49:37 - INFO - Strict retrieval: 0 images from metadata (3 strong, 3 weak links)
```

**ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:** `fetch_images_by_ids()` Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ” 0 images, Ñ…Ğ¾Ñ‡Ğ° metadata Ğ¼Ğ°Ñ” image IDs.

**Ğ§Ğ¾Ğ¼Ñƒ Ñ‚Ğ°Ğº Ğ²Ñ–Ğ´Ğ±ÑƒĞ²Ğ°Ñ”Ñ‚ÑŒÑÑ:**

1. **Metadata Ğ¼Ğ°Ñ” image IDs** (related_image_ids, nearby_image_ids)
2. **fetch_images_by_ids Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ” `.get(where={"image_id": img_id})`**
3. **BUT: metadata Ğ² ChromaDB Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ñ‚ÑŒÑÑ ÑĞº JSON strings!**

```python
# Ğ’ index/build_index.py:
metadata = {
    'image_id': chunk['image_id'],  # âœ… String
    'related_image_ids': json.dumps(chunk['related_image_ids']),  # âŒ JSON string!
}
```

4. **Retriever ÑˆÑƒĞºĞ°Ñ” image_id Ğ² related_image_ids (comma-separated string):**
```python
related = chunk.metadata.get('related_image_ids', '')  # "img1,img2,img3"
if related:
    image_ids_strong.update([id.strip() for id in related.split(',') if id.strip()])
```

5. **ĞĞ»Ğµ fetch_images_by_ids() ÑˆÑƒĞºĞ°Ñ” Ñ‚Ğ¾Ñ‡Ğ½Ğ¸Ğ¹ match:**
```python
results = self.image_store.get(where={"image_id": img_id})  # Exact match fails!
```

---

## ğŸ¯ Recommended Improvements

### **IMPROVEMENT #1: Fix metadata deserialization** 
**Priority:** ğŸ”´ CRITICAL  
**Impact:** +40-50% Image Hit Rate

**Problem:**
`related_image_ids` and `nearby_image_ids` Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°ÑÑ‚ÑŒÑÑ ÑĞº JSON strings Ğ² ChromaDB, Ğ°Ğ»Ğµ Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ÑŒÑÑ ÑĞº strings (Ğ½Ğµ deserializing).

**Solution:**
```python
# In retriever.py - retrieve_with_strict_images():
for chunk in text_chunks:
    # Current (BROKEN):
    related = chunk.metadata.get('related_image_ids', '')
    
    # Fixed:
    related = chunk.metadata.get('related_image_ids', '')
    # If it's a JSON string, deserialize it
    if related and related.startswith('['):
        import json
        related_list = json.loads(related)
        image_ids_strong.update(related_list)
    elif related:
        # Comma-separated fallback
        image_ids_strong.update([id.strip() for id in related.split(',') if id.strip()])
```

**Expected improvement:** Image Hit Rate: 55.6% â†’ 85-90%

---

### **IMPROVEMENT #2: Lower similarity threshold for visual queries**
**Priority:** ğŸŸ  MEDIUM  
**Impact:** +5-10% Image Hit Rate

**Problem:**
Visual queries rely on fallback semantic search with threshold=0.5. ĞœĞ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ Ğ·Ğ°Ğ½Ğ°Ğ´Ñ‚Ğ¾ Ğ¶Ğ¾Ñ€ÑÑ‚ĞºĞ¾.

**Solution:**
```python
# In retriever.py - _fallback_visual_search():
# Current:
is_match, similarity, chunk_id = self.verify_semantic_match(
    img, text_chunks, threshold=0.5, chunk_embeddings=chunk_embeddings
)

# Improved:
is_match, similarity, chunk_id = self.verify_semantic_match(
    img, text_chunks, threshold=0.4,  # Lower for visual queries
    chunk_embeddings=chunk_embeddings
)
```

**Expected improvement:** Image Hit Rate: +5%

---

### **IMPROVEMENT #3: Boost image retrieval for visual keywords**
**Priority:** ğŸŸ¢ LOW  
**Impact:** +2-5% Image Hit Rate

**Problem:**
Non-visual queries ("Explain dropout") Ğ½Ğµ ÑˆÑƒĞºĞ°ÑÑ‚ÑŒ images Ğ½Ğ°Ğ²Ñ–Ñ‚ÑŒ ÑĞºÑ‰Ğ¾ Ğ²Ğ¾Ğ½Ğ¸ Ñ” Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ–.

**Solution:**
```python
# In retriever.py - retrieve_with_verification():
# Add heuristic: if query is NOT visual but metadata has many images, include them

if len(verified_images) == 0 and len(metadata_images) > 0:
    # Even if query not visual, try semantic matching with lower threshold
    # (user may not explicitly ask for images but they're relevant)
    logging.info("  Non-visual query but metadata has images - checking relevance")
    chunk_embeddings = self._batch_embed_chunks(text_chunks)
    image_embeddings = self._batch_embed_images(metadata_images)
    
    verified_images = self._verify_metadata_images(
        metadata_images, text_chunks, chunk_embeddings, image_embeddings
    )
```

**Expected improvement:** Image Hit Rate: +2-5%

---

## ğŸ“‹ Implementation Plan

### **Phase 1: Critical Fix** 
1. âœ… Fix metadata deserialization in `retrieve_with_strict_images()`
2. âœ… Fix metadata deserialization in `_verify_metadata_images()`
3. âœ… Test on 2-3 queries manually
4. âœ… Re-run full evaluation

**Expected after Phase 1:**
- Recall@5: 95% (unchanged)
- Image Hit Rate: 85-90% âœ…
- MRR: 1.0 (unchanged)

### **Phase 2: Fine-tuning** â±ï¸ 15 min
1. Lower threshold for visual query fallback (0.5 â†’ 0.4)
2. Re-run evaluation
3. Adjust if needed

**Expected after Phase 2:**
- Image Hit Rate: 90-95% âœ…

### **Phase 3: Optional Enhancement** â±ï¸ 30 min
1. Add non-visual image relevance check
2. Test on edge cases
3. Document behavior

---

## ğŸš€ Next Steps

**Immediate:**
1. Implement Improvement #1 (metadata deserialization)
2. Re-run evaluation
3. Compare results

**After fixing:**
1. Continue with B1: Faithfulness Judge
2. Complete D: Evaluation Report
3. Document all improvements in final report

---

## ğŸ’¡ Key Insights

### **What Works:**
- âœ… Text retrieval strategy (MMR with Î»=0.7)
- âœ… Semantic verification concept
- âœ… Confidence scoring (HIGH/MEDIUM/LOW)
- âœ… ChromaDB indexing

### **What Needs Work:**
- âŒ Metadata deserialization (JSON strings)
- âŒ Image retrieval for non-visual queries
- âš ï¸ Threshold tuning for different query types

### **Technical Debt:**
- Inconsistency between indexing (JSON.dumps) and retrieval (string parsing)
- Should standardize metadata format across pipeline
- Consider using ChromaDB's native list type instead of JSON strings

---

## ğŸ“ˆ Expected Final Scores

After all improvements:
```
âœ… Recall@5: 95% (target â‰¥70%)
âœ… Image Hit Rate: 90% (target â‰¥60%)
âœ… MRR: 1.0 (target â‰¥0.70)
```

**Result:** All metrics above target! ğŸ‰
