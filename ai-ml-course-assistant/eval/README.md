# Evaluation Directory

This directory contains evaluation scripts and results for Phase E: Production Validation (Completed Jan 19, 2026).

## Structure

```
eval/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ test_queries.json                  # 30 test queries (10 text, 10 visual, 10 hybrid)
â”œâ”€â”€ ground_truth.json                  # Manual ground truth labels with relevance scores
â”œâ”€â”€ evaluate_retrieval.py              # Full retrieval evaluation (Recall@5, MRR, Image Hit Rate)
â”œâ”€â”€ faithfulness_judge.py              # LLM-based faithfulness evaluator
â”œâ”€â”€ validate_ground_truth.py           # Validates ground truth format and coverage
â””â”€â”€ results/                           # Evaluation outputs (.gitignored)
    â”œâ”€â”€ retrieval_eval_YYYYMMDD_HHMMSS.json    # Retrieval metrics
    â”œâ”€â”€ faithfulness_eval_YYYYMMDD_HHMMSS.json # Faithfulness scores
    â””â”€â”€ ...
```

## Current Status (Phase E - Complete)

### âœ… Completed
- `test_queries.json` - 30 test queries (10 text, 10 visual, 10 hybrid)
- `ground_truth.json` - Manual relevance labels for all 30 queries
- `evaluate_retrieval.py` - Full evaluation pipeline for retrieval metrics
- `faithfulness_judge.py` - LLM-based faithfulness validation
- `validate_ground_truth.py` - Ground truth format validation
- All evaluations run and results stored in `results/`

### ðŸ“Š Final Metrics (Jan 19, 2026)
- **Recall@5**: 95% (target: â‰¥70%) âœ…
- **Image Hit Rate**: 88.9% (target: â‰¥60%) âœ…
- **Faithfulness**: 4.525/5.0 (target: â‰¥80%) âœ…
- **MRR**: 1.0 (perfect ranking) âœ…

## Usage

### Run Full Evaluation

Evaluates all 30 test queries against the indexed document collection (54 documents, 369 chunks):

```bash
python eval/evaluate_retrieval.py
```

**Outputs:**
- `results/retrieval_eval_<timestamp>.json` - Retrieval metrics (Recall@5, MRR, Image Hit Rate)
- Console: Detailed per-query results and aggregated statistics

### Run Faithfulness Evaluation

Validates answer faithfulness using LLM-based judgment:

```bash
python eval/faithfulness_judge.py
```

**Outputs:**
- `results/faithfulness_eval_<timestamp>.json` - Faithfulness scores (0-5 scale)
- Console: Per-query faithfulness analysis

### Validate Ground Truth

Checks that ground truth file has correct format and covers all queries:

```bash
python eval/validate_ground_truth.py
```

## Metrics

### Retrieval Metrics (D2)
- **Recall@5**: % of relevant chunks in top-5 (target â‰¥70%)
- **Image Hit Rate**: % of visual queries with â‰¥1 relevant image (target â‰¥60%)
- **MRR (Mean Reciprocal Rank)**: Average 1/rank of first relevant result

### Answer Quality Metrics
- **Faithfulness**: Answer support level (0-5 scale, actual: 4.525/5.0)
- **Citation Accuracy**: % of citations matching retrieved content
- **Context Utilization**: Quality of document context in answers

### Latency Metrics
- Text retrieval time (semantic search)
- Image retrieval time (metadata + verification)
- Total retrieval time
- Generation time (reasoning + answer)
- End-to-end latency

## Test Queries

### Text-focused (10)
Questions about concepts, definitions, explanations without explicit visual requests.

### Visual (10)
Queries explicitly requesting diagrams, figures, architectures, visualizations.

### Hybrid (10)
Queries combining explanations with visual requests ("Explain X and show Y").

See `test_queries.json` for full list.
