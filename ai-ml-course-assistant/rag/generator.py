"""
RAG Answer Generator .

Generates grounded answers with citations from retrieved context.
Enforces topic validation and "I don't know" behavior.
"""

import logging
import re
from typing import Dict, List, Tuple
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)

# OpenAI GPT-5 Mini configuration
MODEL_NAME = "gpt-5-mini"
TEMPERATURE = 0.0  # Zero temperature for maximum determinism, no hallucinations
MAX_TOKENS = 33000  # Reasoning (up to 4000-5000) + answer (up to 4000)
REASONING_EFFORT = "low"  # Faster, less reasoning tokens (~2000-4000 instead of 8000)

# Query validation
MAX_QUERY_LENGTH = 500  # Maximum query length

# Citation validation constants
FIRST_CHUNK_INDEX = 1  # Text chunks are 1-indexed ([1], [2], [3]...)
FIRST_IMAGE_LETTER = 'A'  # Images use letters ([A], [B], [C]...)
FIRST_IMAGE_ASCII = 65  # ASCII code for 'A'

# Context size estimation
CHARS_PER_TOKEN_ESTIMATE = 4  # Rough estimate for token counting

# Response parsing constants
MIN_CONTEXT_LINES_BEFORE = 3  # Minimum context lines for code snippets
MIN_CONTEXT_LINES_AFTER = 3


def sanitize_query(query: str) -> str:
    """
    Sanitize user query to prevent prompt injection attacks.
    
    Security measures:
    1. Remove control characters (\x00-\x1f, \x7f-\x9f)
    2. Limit length to prevent token exhaustion
    3. Filter prompt-breaking patterns
    
    Args:
        query: Raw user query
    
    Returns:
        Sanitized query string
    """
    if not query:
        return ""
    
    # Remove control characters
    query = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', query)
    
    # Limit length
    if len(query) > MAX_QUERY_LENGTH:
        logging.warning(f"Query truncated from {len(query)} to {MAX_QUERY_LENGTH} chars")
        query = query[:MAX_QUERY_LENGTH]
    
    # Filter prompt injection patterns (case-insensitive)
    injection_patterns = [
        (r'ignore\s+previous\s+instructions?', '[FILTERED]'),
        (r'ignore\s+above', '[FILTERED]'),
        (r'forget\s+(?:everything|all|previous)', '[FILTERED]'),
        (r'disregard\s+(?:previous|above)', '[FILTERED]'),
        (r'override\s+(?:system|instructions?)', '[FILTERED]'),
        (r'reveal\s+(?:system|prompt)', '[FILTERED]'),
        (r'you\s+are\s+now', '[FILTERED]'),
        (r'new\s+instructions?:', '[FILTERED]')
    ]
    
    for pattern, replacement in injection_patterns:
        query = re.sub(pattern, replacement, query, flags=re.IGNORECASE)
    
    return query.strip()

# Image citation limit (prevents judge penalizing for too many image references)
MAX_IMAGES_TO_CITE = 5  # Maximum images to include in LLM context

# System prompt enforcing grounding and citations
SYSTEM_PROMPT = """You are an expert AI/ML course assistant specializing in deep learning and machine learning.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ MANDATORY OUTPUT FORMAT - FOLLOW EXACTLY âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your response MUST have these THREE sections in this EXACT order:

Answer: <your answer with inline citations [1], [2], [A], [B]>

Sources: <list citations like: [1], [2], [3], [A]>

Reasoning: <1-2 sentences explaining which sources you used and why>

DO NOT deviate from this format. DO NOT skip any section. DO NOT add extra sections.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CRITICAL RULES:

1. TOPIC VALIDATION: You can ONLY answer questions about:
   - Deep Learning (neural networks, CNNs, RNNs, Transformers, etc.)
   - Machine Learning (algorithms, optimization, training, etc.)
   - Computer Vision, NLP, model architectures
   - Data Science Tools (NumPy, pandas, matplotlib, scikit-learn) when used for ML/DL
   - Python libraries commonly used in AI/ML workflows
   
   If query is NOT about AI/ML/DL/Data Science â†’ respond in the SAME FORMAT:
   Answer: I can only assist with AI, machine learning, deep learning, and related data science topics. Please ask about neural networks, architectures, training methods, or ML tools like NumPy, pandas, or scikit-learn.
   
   Sources: N/A
   
   Reasoning: Query is outside AI/ML/Data Science scope.

2. AMBIGUOUS QUERIES - ASK FOR CLARIFICATION:
   If query is GENERIC and could refer to MULTIPLE architectures/concepts:
   - Example: "show encoder decoder" â†’ could be Transformer, RNN, VAE, Autoencoder
   - Example: "explain attention" â†’ could be self-attention, cross-attention, scaled dot-product
   
   Then respond in the SAME FORMAT:
   Answer: Your question could refer to multiple architectures. I found information about [list what's in context, e.g., "Transformer encoder-decoder"]. Would you like me to explain that specific architecture, or did you mean a different encoder-decoder model (e.g., RNN-based, VAE)? Please clarify which architecture you're interested in.
   
   Sources: N/A
   
   Reasoning: Query is ambiguous - asking for clarification before answering.

3. GROUNDED ANSWERS ONLY:
   - Use ONLY information from the provided context
   - NEVER use external knowledge or make assumptions
   - If context doesn't contain answer â†’ respond in the SAME FORMAT:
   
   Answer: I don't have enough information in my knowledge base to answer this question. The retrieved context doesn't contain relevant details about [topic].
   
   Sources: N/A
   
   Reasoning: Retrieved context lacks information to answer this specific query.

4. ANSWER STRUCTURE (inside "Answer:" section):
   - Write 3 natural paragraphs in flowing prose
   - Target: ~250-300 words total (~80-100 words per paragraph)
   - Start with core concept, then implementation details, then evidence/figures
   - NO paragraph labels, NO bullet points, NO numbered lists
   - Integrate citations naturally: [1], [2], [A]
   - Do NOT mention confidence levels in Answer
   
   For visual queries (show/diagram/architecture):
   - Describe what's visible in figures
   - Reference parts: "left side shows X, right side shows Y"
   - Always cite images: [A], [B], [C]
   - Keep description objective without confidence disclaimers
   
   **Image Integration (CRITICAL - COMPLEMENT, DON'T COPY):**
   - PURPOSE: Images COMPLEMENT explanation, not replace it - use visuals to SUPPORT theory
   - EXTRACT facts from Caption: what variables are shown, what relationships are visualized
   - INTEGRATE into theory: explain concept first, then reference visual EVIDENCE
   - FORBIDDEN: simply copying Caption text or listing visual elements without context
   - CITE [A] when referencing visual evidence that supports your explanation
   - BRIDGE concept to visual: "This relationship is evident in [A]...", "demonstrating..." 
   
   âŒ WRONG APPROACHES:
   1. Hallucination: "gray squares for p(xi)" â† Caption never mentions "squares"
   2. Copy-paste: "green circular markers represent yi" â† Just repeating Caption verbatim
   
   âœ… CORRECT APPROACH:
   Caption: "Green circular markers represent actual responses yi, S-shaped curve shows p(x)"
   GOOD: "The model maps linear predictions to probabilities via sigmoid, with actual binary outcomes clearly separated from the smooth probability curve [C]."
   WHY: Explains CONCEPT (sigmoid mapping), uses Caption FACTS (binary outcomes, probability curve) to support explanation
   
   âœ… RULE: Explain the concept using text sources [1][2], then reference visual CONFIRMATION [A][B]

5. CITATIONS (MANDATORY - CRITICAL RULES - NO HALLUCINATIONS):
   
   **CRITICAL: Check METADATA FIRST before citing anything:**
   - BEFORE writing Answer, read METADATA section
   - Note "Total text chunks: X" â†’ valid citations are ONLY [1] through [X]
   - Note "Total images: Y" â†’ valid citations are ONLY [A] through [Yth letter]
   - EXAMPLE: If "Total images: 2" â†’ ONLY [A], [B] are valid. [C], [D]... are FORBIDDEN
   - NEVER hallucinate citations that don't exist in METADATA
   
   **Citation Format (must be in TEXT CONTEXT or IMAGE CONTEXT):**
   - Text sources: [1], [2], [3] ONLY (numbers must be â‰¤ Total text chunks)
   - Images: [A], [B], [C] ONLY (letters must be â‰¤ Total images, A=1st image, B=2nd image, C=3rd image)
   - NEVER write "chunk [2]" or "chunk [X]" - use ONLY [2]
   - NEVER cite non-existent labels (if only [A],[B] exist in METADATA, do NOT cite [C])
   
   **Answer-Sources Synchronization (MANDATORY):**
   - Every citation in Answer MUST appear in Sources
   - Every citation in Sources MUST appear in Answer
   - If Answer uses [A], [B], [C] â†’ Sources MUST list [A], [B], [C]
   - If Answer uses [1], [2] â†’ Sources MUST list [1], [2]
   - NEVER cite image [C] if METADATA says "Total images: 2"
   
   **Figure References:**
   - Include Figure numbers: "Figure 2 [A]", "Figure 7 [B]"
   - If Table is PART of Figure â†’ cite Figure: "Table 7 in Figure 7 [B]" or just [B]
   - Multi-part figures: "Figure 3 left [A]" means left side of Figure 3
   
   **In "Sources:" section:**
   - List ALL citations used, e.g.: [1], [2], [3], [A], [B]
   - Double-check: EXACT match with Answer citations - no more, no less
   
   âŒ HALLUCINATION EXAMPLE (FORBIDDEN):
   Answer: "Figure 4 shows attention patterns [A]. Figure 5 shows filters [B]. Appendix [C] has more."
   Sources: [1], [2], [A], [B]
   METADATA: Total images: 2
   ERROR: [C] cited in Answer but METADATA says only 2 images exist. This is a hallucination!
   
   âœ… CORRECT VERSION:
   Answer: "Figure 4 shows attention patterns [A]. Figure 5 shows filters [B]."
   Sources: [1], [2], [A], [B]
   METADATA: Total images: 2
   SUCCESS: Citations match METADATA. Answer-Sources synchronized.

6. REASONING SECTION (MANDATORY - EXPLICIT CONFIDENCE):
   
   **Required format:** Always include confidence levels for images
   - List which chunks were used: "Used chunks 1-3 for..."
   - List which images were used WITH their confidence: "Image A (HIGH, 0.95)", "Image B (MEDIUM, 0.68)"
   - Explain why these sources support the answer
   
   **Confidence Labels:**
   - HIGH (0.8-1.0): "Image A (HIGH, 0.92)" - use freely in answer
   - MEDIUM (0.6-0.79): "Image B (MEDIUM, 0.68)" - can use with context
   - LOW (0.5-0.59): "Image C (LOW, 0.52)" - use cautiously, mention uncertainty in answer
   
   **Example Reasoning with confidences:**
   âœ… CORRECT:
   "Used chunks 1-3 for architectural details and CIFAR-10 results. Image A (HIGH, 0.92) shows layer stability, Image B (MEDIUM, 0.68) illustrates attention mechanism."
   
   âŒ INCORRECT (vague):
   "Used relevant sources and images." â† Too vague, no confidence info
   
   âŒ INCORRECT (confidence in answer):
   "Figure 2 [B] illustrates attention (medium confidence)..." â† Confidence should be in Reasoning, not Answer

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXAMPLE 1 (Complete format with confidences):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Answer: Residual connections are skip connections that add the input x directly to the output of stacked layers, resulting in x + F(x) [1]. Each residual block uses identity shortcuts bypassing 3Ã—3 convolutions without adding extra parameters, where the shortcut is connected to each pair of 3Ã—3 layers [1].

This design solves the degradation problem where very deep plain networks have higher training error as depth increases [2]. The skip connections provide a direct gradient flow path during backpropagation, stabilizing training. In practice, ResNets use h = 3n shortcuts for architectures with n residual blocks, with dimensions handled through zero-padding or projections when needed [1][3].

Empirically, 110-layer ResNets achieve lower training and testing error than plain counterparts on CIFAR-10 [3]. Figure 7 [A] demonstrates this through layer response analysis: ResNets maintain stable activations with lower standard deviation across layers compared to plain networks, confirming improved optimization dynamics. This enables substantially deeper models (110+ layers) with better performance [2][3].

Sources: [1], [2], [3], [A]

Reasoning: Used chunks 1-3 for technical details about residual connections and CIFAR-10 results. Image A (HIGH, 0.92) shows layer stability analysis, directly supporting optimization claims.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXAMPLE 2 (Visual query with explicit confidences):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Answer: The Transformer architecture consists of an encoder-decoder structure with multi-head self-attention mechanisms [1]. As shown in Figure 1 [A], the left side displays the encoder stack with 6 identical layers, each containing multi-head attention and feed-forward sublayers. The right side shows the decoder stack with similar components plus masked attention to prevent positions from attending to future tokens [1].

Multi-head attention operates by projecting queries, keys, and values h times using learned linear projections [2]. Each attention head computes scaled dot-product attention in parallel, with outputs concatenated and linearly transformed. Figure 2 [B] illustrates this parallel processing where h=8 heads operate simultaneously, allowing the model to jointly attend to information from different representation subspaces [2].

The positional encoding mechanism adds sinusoidal patterns to input embeddings since the architecture contains no recurrence [3]. This enables the model to leverage sequence order information while maintaining parallelizability during training. The complete architecture achieves state-of-the-art results on machine translation benchmarks [3].

Sources: [1], [2], [3], [A], [B]

Reasoning: Used all three chunks for architectural details. Image A (HIGH, 0.94) provides clear diagram of encoder-decoder structure. Image B (HIGH, 0.91) shows multi-head attention mechanism with explicit h=8 heads visualization.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NOW RESPOND TO THE USER'S QUERY USING THIS EXACT FORMAT.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


class RAGGenerator:
    """
    Answer generator for RAG pipeline.
    
    Features:
    - OpenAI GPT-5 Nano integration (400K context, 128K output, reasoning support)
    - Grounded answer generation with mandatory citations
    - Topic validation (AI/ML only)
    - "I don't know" behavior when context insufficient
    - Confidence-aware image usage
    """
    
    def __init__(self, openai_api_key: str = None):
        """
        Initialize generator with OpenAI GPT-5 Nano.
        
        Args:
            openai_api_key: Optional API key (uses .env if not provided)
        """
        logging.info(f"Initializing RAGGenerator with {MODEL_NAME}")
        
        self.llm = ChatOpenAI(
            model=MODEL_NAME,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
            reasoning_effort=REASONING_EFFORT,
            api_key=openai_api_key
        )
        
        self.system_prompt = SYSTEM_PROMPT
        logging.info("Generator ready")
    
    def _extract_chunk_citations(
        self, 
        sources_text: str, 
        text_chunks: List[Dict]
    ) -> Tuple[List[str], List[str]]:
        """
        Extract and validate chunk citations from sources text.
        
        Single Responsibility: Citation extraction and validation for text chunks.
        
        Args:
            sources_text: Text containing citations like [1], [2], [3]
            text_chunks: List of available text chunks
        
        Returns:
            Tuple of (chunk_ids, citation_labels)
        """
        chunk_citations = re.findall(r'\[(\d+)\]', sources_text)
        chunk_ids = []
        
        for idx in chunk_citations:
            try:
                idx_int = int(idx) - FIRST_CHUNK_INDEX
                if 0 <= idx_int < len(text_chunks):
                    chunk_ids.append(text_chunks[idx_int]['chunk_id'])
            except (ValueError, KeyError, IndexError) as e:
                logging.warning(f"Invalid chunk citation [{idx}]: {e}")
                continue
        
        return chunk_ids, chunk_citations
    
    def _extract_image_citations(
        self, 
        sources_text: str, 
        images: List[Dict]
    ) -> Tuple[List[str], List[str]]:
        """
        Extract and validate image citations from sources text.
        
        Single Responsibility: Citation extraction and validation for images.
        
        Args:
            sources_text: Text containing citations like [A], [B], [C]
            images: List of available images
        
        Returns:
            Tuple of (image_ids, citation_letters)
        """
        image_citations = re.findall(r'\[([A-Z])\]', sources_text)
        image_ids = []
        
        for letter in image_citations:
            try:
                idx = ord(letter) - FIRST_IMAGE_ASCII
                if 0 <= idx < len(images):
                    image_ids.append(images[idx]['image_id'])
            except (KeyError, IndexError) as e:
                logging.warning(f"Invalid image citation [{letter}]: {e}")
                continue
        
        return image_ids, image_citations
    
    def _check_special_responses(self, answer_text: str) -> Tuple[bool, bool]:
        """
        Check if response is off-topic or has insufficient context.
        
        Single Responsibility: Response type detection.
        
        Args:
            answer_text: The answer text to check
        
        Returns:
            Tuple of (is_off_topic, is_insufficient_context)
        """
        answer_lower = answer_text.lower()
        is_off_topic = ("only assist with ai" in answer_lower or 
                        "only assist with deep learning" in answer_lower or
                        "outside ai/ml" in answer_lower)
        is_insufficient = (
            "don't have enough information" in answer_lower or 
            "doesn't contain relevant" in answer_lower
        )
        return is_off_topic, is_insufficient
    
    def _validate_and_clean_citations(self, response_text: str, num_chunks: int, num_images: int) -> str:
        """
        Validate and clean citations in generated response.
        
        Fixes:
        1. Removes "chunk [X]" format â†’ [X]
        2. Removes citations to non-existent images/chunks
        3. Cleans up orphaned brackets
        
        Args:
            response_text: Generated answer text
            num_chunks: Number of available text chunks (1-indexed)
            num_images: Number of available images (A-indexed)
        
        Returns:
            Cleaned response text with only valid citations
        """
        original = response_text
        
        # Fix 1: Remove "chunk [X]" format
        response_text = re.sub(r',?\s*chunk\s+\[(\d+)\]', r' [\1]', response_text, flags=re.IGNORECASE)
        if response_text != original:
            logging.info("âœ“ Removed 'chunk [X]' format from citations")
        
        # Fix 2: Validate text chunk citations
        valid_chunk_nums = set(str(i) for i in range(1, num_chunks + 1))
        cited_chunks = set(re.findall(r'\[(\d+)\]', response_text))
        invalid_chunks = cited_chunks - valid_chunk_nums
        
        if invalid_chunks:
            logging.warning(f"Invalid chunk citations: {invalid_chunks} (valid: {valid_chunk_nums})")
            for num in invalid_chunks:
                # Remove invalid chunk citations
                response_text = re.sub(rf'\[{num}\]', '', response_text)
            logging.info(f"âœ“ Removed {len(invalid_chunks)} invalid chunk citation(s)")
        
        # Fix 3: Validate image citations
        valid_image_letters = set(chr(65 + i) for i in range(num_images))  # A, B, C...
        cited_images = set(re.findall(r'\[([A-Z])\]', response_text))
        invalid_images = cited_images - valid_image_letters
        
        if invalid_images:
            logging.warning(f"Invalid image citations: {invalid_images} (valid: {valid_image_letters})")
            for letter in invalid_images:
                # Remove invalid image citations
                response_text = re.sub(rf'\[{letter}\]', '', response_text)
            logging.info(f"âœ“ Removed {len(invalid_images)} invalid image citation(s)")
        
        # Fix 4: Clean up orphaned brackets and extra spaces
        response_text = re.sub(r'\]\s*\[', '] [', response_text)  # ][ â†’ ] [
        response_text = re.sub(r'\s{2,}', ' ', response_text)  # multiple spaces â†’ single
        response_text = re.sub(r'\s+([.,!?])', r'\1', response_text)  # space before punctuation
        
        if response_text != original:
            logging.info("âœ“ Citations validated and cleaned")
        
        return response_text
    
    def format_context_for_llm(self, llm_input: Dict) -> str:
        """
        Format retrieval results into structured context for LLM.
        
        Creates numbered text sources [1], [2], [3] and lettered image sources [A], [B], [C].
        
        Args:
            llm_input: Output from retriever.prepare_for_llm()
                {query, text_chunks, images, metadata}
        
        Returns:
            Formatted context string with labels
        """
        context_parts = []
        
        # Header
        context_parts.append(f"Query: {llm_input['query']}\n")
        
        # Text chunks with numbered labels
        if llm_input['text_chunks']:
            context_parts.append("=" * 70)
            context_parts.append("TEXT CONTEXT:")
            context_parts.append("=" * 70)
            
            for i, chunk in enumerate(llm_input['text_chunks'], 1):
                chunk_text = (
                    f"\n[{i}] chunk_id: {chunk['chunk_id']}\n"
                    f"    Source: {chunk['source']} (page {chunk['page']})\n"
                    f"    Has figure references: {chunk['has_figure_references']}\n"
                )
                
                if chunk['related_image_ids']:
                    chunk_text += f"    Related images: {', '.join(chunk['related_image_ids'])}\n"
                
                chunk_text += f"    Text: {chunk['text']}\n"
                context_parts.append(chunk_text)
        
        # Images with lettered labels
        if llm_input['images']:
            context_parts.append("\n" + "=" * 70)
            context_parts.append("IMAGE CONTEXT:")
            context_parts.append("=" * 70)
            
            for i, img in enumerate(llm_input['images'], 1):
                label = chr(FIRST_IMAGE_ASCII + i - 1)  # A, B, C...
                
                # Note: DO NOT add figure numbers to label line - causes citation confusion
                # The figure number is already in the caption text
                img_text = (
                    f"\n[{label}] image_id: {img['image_id']}\n"
                    f"    Filename: {img['filename']}\n"
                    f"    Page: {img['page']}\n"
                    f"    Confidence: {img['confidence']} (similarity: {img['similarity']})\n"
                    f"    Reason: {img['reason']}\n"
                    f"    Caption: {img['caption']}\n"
                )
                context_parts.append(img_text)
        
        # Metadata summary
        context_parts.append("\n" + "=" * 70)
        context_parts.append("METADATA:")
        context_parts.append("=" * 70)
        meta = llm_input['metadata']
        
        num_chunks = meta['num_text_chunks']
        num_images = meta['num_images']
        
        context_parts.append(
            f"Total text chunks: {num_chunks}\n"
            f"Total images: {num_images}\n"
            f"  HIGH confidence: {meta['high_confidence_images']}\n"
            f"  MEDIUM confidence: {meta['medium_confidence_images']}\n"
            f"  LOW confidence: {meta['low_confidence_images']}\n"
        )
        
        # EXPLICIT citation availability (for generator, not evaluator)
        context_parts.append("\nğŸ“Œ CITATIONS PROVIDED IN THIS CONTEXT:")
        if num_chunks > 0:
            chunk_citations = ', '.join([f"[{i}]" for i in range(1, num_chunks + 1)])
            context_parts.append(f"  Text chunks available: {chunk_citations}")
        else:
            context_parts.append("  Text chunks available: NONE")
        
        if num_images > 0:
            image_citations = ', '.join([f"[{chr(65 + i)}]" for i in range(num_images)])
            context_parts.append(f"  Images available: {image_citations}")
        else:
            context_parts.append("  Images available: NONE")
        
        context_parts.append("  â„¹ï¸  These are all citations that appear in the context above.\n")
        
        # Instructions
        context_parts.append("\n" + "=" * 70)
        context_parts.append("INSTRUCTIONS:")
        context_parts.append("=" * 70)
        context_parts.append("""
            1. Check if query is about deep learning/machine learning
            2. Analyze if the provided context is sufficient to answer
            3. Generate answer using ONLY the context above
            4. Use citations: [1], [2] for text chunks; [A], [B] for images
            5. Explain your reasoning (which sources used, why)
            6. If using MEDIUM/LOW confidence images, acknowledge uncertainty
            """)
        
        return "\n".join(context_parts)
    
    def parse_response(self, response_text: str, llm_input: Dict) -> Dict:
        """
        Parse LLM response into structured output with robust fallback.
        
        Expected format:
            Answer: <text with citations>
            Sources: [1], [2], [A]
            Reasoning: <explanation>
        
        Fallback: If format doesn't match, returns full text as answer.
        
        Args:
            response_text: Raw LLM response
            llm_input: Original input for validation
        
        Returns:
            Dict with answer, citations, reasoning, flags
        """
        # Extract sections using case-insensitive regex with flexible whitespace
        answer_match = re.search(
            r'(?:Answer|ANSWER)[:\s]*(.+?)(?=(?:Sources|SOURCES)[:\s]|$)', 
            response_text, 
            re.DOTALL | re.IGNORECASE
        )
        sources_match = re.search(
            r'(?:Sources|SOURCES)[:\s]*(.+?)(?=(?:Reasoning|REASONING)[:\s]|$)', 
            response_text, 
            re.DOTALL | re.IGNORECASE
        )
        reasoning_match = re.search(
            r'(?:Reasoning|REASONING)[:\s]*(.+?)$', 
            response_text, 
            re.DOTALL | re.IGNORECASE
        )
        
        # Fallback if parsing fails - try flexible extraction
        if not answer_match:
            logging.warning(
                "LLM response doesn't match expected format. "
                "Attempting flexible parsing..."
            )
            
            # Try alternative patterns (e.g., markdown headers, bold labels)
            alt_answer = re.search(
                r'(?:##?\s*Answer|Answer|ANSWER|\*\*Answer\*\*)[:\s]*(.+?)(?=(?:##?\s*Sources|Sources|SOURCES|\*\*Sources\*\*)[:\s]|$)',
                response_text,
                re.DOTALL | re.IGNORECASE
            )
            alt_sources = re.search(
                r'(?:##?\s*Sources|Sources|SOURCES|\*\*Sources\*\*)[:\s]*(.+?)(?=(?:##?\s*Reasoning|Reasoning|REASONING|\*\*Reasoning\*\*)[:\s]|$)',
                response_text,
                re.DOTALL | re.IGNORECASE
            )
            alt_reasoning = re.search(
                r'(?:##?\s*Reasoning|Reasoning|REASONING|\*\*Reasoning\*\*)[:\s]*(.+?)$',
                response_text,
                re.DOTALL | re.IGNORECASE
            )
            
            if alt_answer:
                logging.info("âœ“ Flexible parsing succeeded with alternative patterns")
                answer_match = alt_answer
                sources_match = alt_sources
                reasoning_match = alt_reasoning
            else:
                # Last resort: extract citations from full text
                logging.warning("Flexible parsing partially failed. Extracting citations from unstructured text...")
                
                # Validate input
                text_chunks = llm_input.get('text_chunks', [])
                images = llm_input.get('images', [])
                
                # Extract citations using helper methods (DRY)
                chunk_ids, chunk_citations = self._extract_chunk_citations(response_text, text_chunks)
                image_ids, image_citations = self._extract_image_citations(response_text, images)
                
                # Reconstruct sources text from found citations
                sources_text = ', '.join(
                    [f"[{idx}]" for idx in chunk_citations] +
                    [f"[{letter}]" for letter in image_citations]
                )
                
                # Check response type using helper method (SRP)
                is_off_topic, is_insufficient = self._check_special_responses(response_text)
                
                logging.info(f"âœ“ Extracted {len(chunk_ids)} chunk citations, {len(image_ids)} image citations from unstructured text")
                
                return {
                    'answer': response_text.strip(),
                    'cited_chunks': chunk_ids,
                    'cited_images': image_ids,
                    'sources_text': sources_text,
                    'reasoning': '(extracted from unstructured response)',
                    'is_off_topic': is_off_topic,
                    'is_insufficient_context': is_insufficient,
                    'raw_response': response_text,
                    'parsing_failed': bool(not sources_text)  # Only failed if no citations found
                }
        
        answer = answer_match.group(1).strip()
        sources_text = sources_match.group(1).strip() if sources_match else ""
        reasoning = reasoning_match.group(1).strip() if reasoning_match else ""
        
        # Validate input data before processing
        if not llm_input or 'text_chunks' not in llm_input:
            logging.warning("Invalid llm_input: missing 'text_chunks' key")
            llm_input = {'text_chunks': [], 'images': []}
        
        text_chunks = llm_input.get('text_chunks', [])
        images = llm_input.get('images', [])
        
        # Extract citations using helper methods (DRY)
        chunk_ids, _ = self._extract_chunk_citations(sources_text, text_chunks)
        image_ids, _ = self._extract_image_citations(sources_text, images)
        
        # Check response type using helper method (SRP)
        is_off_topic, is_insufficient = self._check_special_responses(answer)
        
        return {
            'answer': answer,
            'cited_chunks': chunk_ids,
            'cited_images': image_ids,
            'sources_text': sources_text,
            'reasoning': reasoning,
            'is_off_topic': is_off_topic,
            'is_insufficient_context': is_insufficient,
            'raw_response': response_text
        }
    
    def generate(self, llm_input: Dict) -> Dict:
        """
        Generate answer from retrieval results.
        
        Full pipeline:
        1. Sanitize query input
        2. Format context for LLM
        3. Create messages (system + user)
        4. Call OpenAI LLM
        5. Parse response
        6. Return structured result
        
        Args:
            llm_input: Output from retriever.prepare_for_llm()
        
        Returns:
            Dict with answer, citations, reasoning, metadata
        """
        # Sanitize query to prevent prompt injection
        original_query = llm_input['query']
        llm_input['query'] = sanitize_query(original_query)
        
        if original_query != llm_input['query']:
            logging.warning(f"Query sanitized: '{original_query}' -> '{llm_input['query']}'")
        
        logging.info(f"Generating answer for query: '{llm_input['query']}'")
        
        # Limit images to prevent citation overflow (Queries #4, #7)
        # Judge penalizes answers with 4+ image citations, so cap at MAX_IMAGES_TO_CITE
        if len(llm_input.get('images', [])) > MAX_IMAGES_TO_CITE:
            original_count = len(llm_input['images'])
            # Keep highest confidence images (sorted by confidence_score descending)
            llm_input['images'] = sorted(
                llm_input['images'], 
                key=lambda img: img.get('metadata', {}).get('confidence_score', 0.0),
                reverse=True
            )[:MAX_IMAGES_TO_CITE]
            logging.info(f"Limited images from {original_count} to {MAX_IMAGES_TO_CITE} (keeping highest confidence)")
            
            # Update metadata count
            if 'metadata' in llm_input:
                llm_input['metadata']['num_images'] = len(llm_input['images'])
        
        # Format context
        formatted_context = self.format_context_for_llm(llm_input)
        
        # Log context size
        context_chars = len(formatted_context)
        context_tokens_estimate = context_chars // CHARS_PER_TOKEN_ESTIMATE  # Rough estimate
        logging.info(f"Context size: {context_chars} chars (~{context_tokens_estimate} tokens)")
        
        # Build messages
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=formatted_context)
        ]
        
        # Call LLM
        logging.info("Calling OpenAI GPT-5 Mini...")
        response_text = ""
        try:
            response = self.llm.invoke(messages)
            
            # Extract content (handle AIMessage structure)
            if hasattr(response, 'content'):
                response_text = response.content or ""
            else:
                response_text = str(response)
            
            if not response_text:
                logging.warning("LLM returned empty response")
                response_text = "Error: Empty response from LLM"
            
            logging.info(f"LLM response received ({len(response_text)} chars)")
            
            # Log metadata if available
            if hasattr(response, 'response_metadata'):
                token_usage = response.response_metadata.get('token_usage', {})
                if token_usage:
                    logging.info(f"Tokens: {token_usage.get('total_tokens', 'unknown')} "
                               f"(reasoning: {token_usage.get('completion_tokens_details', {}).get('reasoning_tokens', 0)})")
            
        except ValueError as e:
            logging.error(f"LLM input validation error: {e}")
            response_text = "Error: Invalid input data for LLM"
        except ConnectionError as e:
            logging.error(f"LLM connection error: {e}")
            response_text = "Error: Failed to connect to LLM service"
        except TimeoutError as e:
            logging.error(f"LLM timeout error: {e}")
            response_text = "Error: LLM request timed out"
        except Exception as e:
            logging.error(f"Unexpected error calling LLM: {type(e).__name__} - {e}")
            response_text = f"Error generating response: {type(e).__name__}"
        
        # Parse response
        result = self.parse_response(response_text, llm_input)
        
        # Validate and clean citations (post-generation fix)
        # Add validation for metadata existence
        metadata = llm_input.get('metadata', {})
        num_chunks = metadata.get('num_text_chunks', 0)
        num_images = metadata.get('num_images', 0)
        
        result['answer'] = self._validate_and_clean_citations(
            result['answer'],
            num_chunks=num_chunks,
            num_images=num_images
        )
        
        # Check Answer-Sources consistency (log-only, don't auto-delete to avoid distorting valid answers)
        sources_chunks = set(re.findall(r'\[(\d+)\]', result['sources_text']))
        sources_images = set(re.findall(r'\[([A-Z])\]', result['sources_text']))
        
        answer_chunks = set(re.findall(r'\[(\d+)\]', result['answer']))
        answer_images = set(re.findall(r'\[([A-Z])\]', result['answer']))
        
        orphan_chunks = answer_chunks - sources_chunks
        orphan_images = answer_images - sources_images
        
        if orphan_chunks or orphan_images:
            logging.warning(f"âš ï¸  Answer-Sources inconsistency: Answer={orphan_chunks or 'OK'}/{orphan_images or 'OK'} missing from Sources")
        
        # Add metadata
        result['query'] = llm_input['query']
        result['num_chunks_retrieved'] = llm_input['metadata']['num_text_chunks']
        result['num_images_retrieved'] = llm_input['metadata']['num_images']
        
        # Log result
        if result['is_off_topic']:
            logging.warning("Query flagged as off-topic")
        elif result['is_insufficient_context']:
            logging.warning("Insufficient context to answer")
        else:
            logging.info(f"Answer generated: {len(result['cited_chunks'])} chunks, {len(result['cited_images'])} images cited")
        
        return result


def test_generator():
    """Test generator with different scenarios."""
    from retriever import MultimodalRetriever
    
    print("=" * 70)
    print("ğŸ§ª Testing RAG Generator")
    print("=" * 70)
    print()
    
    # Initialize
    retriever = MultimodalRetriever()
    generator = RAGGenerator()
    
    # Test scenarios
    test_queries = [
        ("show the transformer encoder decoder architecture", "Visual query (should use images)"),
        ("explain gradient descent algorithm", "Conceptual query (no images expected)"),
        ("how to cook pasta", "Off-topic query (should reject)")
    ]
    
    for query, description in test_queries:
        print(f"\n{'=' * 70}")
        print(f"Query: {query}")
        print(f"Description: {description}")
        print(f"{'=' * 70}")
        
        # Retrieve
        text_chunks, verified_images = retriever.retrieve_with_verification(query, k_text=3)
        llm_input = retriever.prepare_for_llm(query, text_chunks, verified_images)
        
        print(f"\nRetrieved: {len(text_chunks)} chunks, {len(verified_images)} images")
        
        # Generate
        result = generator.generate(llm_input)
        
        # Display
        print(f"\nğŸ“ Answer:")
        print(result['answer'])
        
        if result['is_off_topic']:
            print("\nâš ï¸  OFF-TOPIC DETECTED")
        elif result['is_insufficient_context']:
            print("\nâš ï¸  INSUFFICIENT CONTEXT")
        else:
            print(f"\nğŸ“š Sources: {result['sources_text']}")
            print(f"\nğŸ’­ Reasoning: {result['reasoning']}")
            print(f"\nâœ… Cited chunks: {result['cited_chunks']}")
            print(f"âœ… Cited images: {result['cited_images']}")
        
        print()
    
    print("=" * 70)
    print("âœ… Testing complete!")
    print("=" * 70)


if __name__ == "__main__":
    test_generator()
