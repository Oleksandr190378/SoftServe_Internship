# Roadmap: MVP ‚Üí Production System

**Project:** AI/ML Course Assistant - Multimodal RAG  
**Status:** MVP Complete, Scaling to 54 Documents  
**Last Updated:** January 3, 2026

---

## Overview

**Current State:**
- ‚úÖ MVP working with 3 arXiv papers (VGG, ResNet, Attention)
- ‚úÖ 104 text chunks, 9 images in ChromaDB
- ‚úÖ Generator with GPT-5 Nano (reasoning support)
- ‚úÖ Optimized prompt engineering (quality 9-9.5/10)
- ‚úÖ Streamlit UI functional

**Target State:**
- üéØ 20-25 documents from diverse sources
- üéØ 500-700 text chunks, 150-250 images
- üéØ Incremental indexing pipeline
- üéØ Quantitative evaluation metrics
- üéØ Production-ready codebase

---

## Phase A: Code Cleanup & Refactoring üìù

**Duration:** 1-2 days  
**Priority:** üî¥ HIGH  
**Status:** üîÑ IN PROGRESS (Started Jan 2, 2026)

### Architecture Decision: 2-Stage Pipeline

**Stage 1: Data Collection** üì•
- User downloads raw documents from sources
- Manual curation/review before processing
- Output: `data/raw/{source}/{doc_id}/`

**Stage 2: Processing & Indexing** ‚öôÔ∏è
- Automated pipeline: Extract ‚Üí Caption ‚Üí Chunk ‚Üí Embed ‚Üí Index
- Incremental: Only new documents (check `processed_docs.json`)
- Output: ChromaDB collections

### Tasks:

- [x] **A0. Clean test data** ‚úÖ
  - [x] Deleted `data/chroma_db/` (test ChromaDB)
  - [x] Deleted `data/processed/` (intermediate files)
  - [x] Deleted test scripts: clean_metadata.py, analyze_openai_results.py, test_extraction.py
  - [x] Ready for fresh start

- [x] **A1. Refactor download scripts (Stage 1)** ‚úÖ
  
  **A1.1. download_arxiv.py** ‚úÖ DONE
  - [x] Replace all `print()` ‚Üí `logging.info/debug/error`
  - [x] Keep only essential CLI in `__main__`
  - [x] Add function: `download_papers(paper_ids: List[str]) -> List[str]`
  - [x] Return downloaded doc_ids for tracking
  - [x] 35 curated papers with complete DL coverage
  
  **A1.2. download_realpython.py** ‚úÖ DONE
  - [x] BeautifulSoup for HTML scraping
  - [x] Function: `download_articles(urls: List[str]) -> List[str]`
  - [x] Extract code blocks + images with captions
  - [x] Save: `data/raw/realpython/{slug}/{slug}.json`
  - [x] 9 articles downloaded (316 code blocks, 115 images)
  
  **A1.3. download_medium.py** ‚úÖ DONE
  - [x] BeautifulSoup for HTML scraping
  - [x] Function: `download_articles(urls: List[str]) -> List[str]`
  - [x] Multi-selector fallback for TDS (article ‚Üí article-content ‚Üí post-content ‚Üí body)
  - [x] Image caption parsing with figcaption support
  - [x] Spam filtering (recommended stories, follow buttons)
  - [x] Save: `data/raw/medium/{slug}/{slug}.json`
  - [x] 10 articles downloaded (RAG evaluation, chunking, agents, metrics)
  
  **A1.4. download_docs.py** ‚ùå SKIPPED
  - Official docs not included in dataset (0 pages in PRD)

- [x] **A2. Refactor processing scripts (Stage 2)** ‚úÖ DONE (Jan 3)
  
  **A2.1. extract_images_smart.py + extract_from_json.py** ‚úÖ DONE
  - [x] PDF: extract_document(doc_id) returns Dict with images/text
  - [x] JSON: extract_json_document(doc_id) for RealPython/Medium
  - [x] Context extraction: 200 chars before/after images
  - [x] Dual strategy: caption search + position estimation
  
  **A2.2. enrich_images.py** ‚úÖ DONE
  - [x] generate_captions_for_doc(doc_id, use_vlm) function
  - [x] Enhanced Vision LLM prompt (8 categories: academic + IDE + workflows)
  - [x] Web images support (uses stored context from JSON)
  - [x] Tested on 4 images with excellent results (avg 410 words)
  
  **A2.3. chunk_documents.py** ‚úÖ DONE
  - [x] Replaced print ‚Üí logging
  - [x] chunk_document_with_image_tracking(doc_id, full_text, ...)
  - [x] PDF: page-based linking (page_num)
  - [x] JSON: position-based linking (image_index + relative position)
  - [x] Params: chunk_size=1800 (~500 tokens), overlap=200 (~60 tokens)
  - [x] Tested on 3 docs: 1 PDF (22 chunks) + 2 JSON (5+22 chunks)
  
  **A2.4. index/embedding_utils.py** ‚úÖ DONE (Jan 4)
  - [x] Replace print ‚Üí logging ‚úÖ
  - [x] Add function: `embed_document(doc_id: str) -> Dict` ‚úÖ
  - [x] Model: text-embedding-3-small (1536 dims)
  - [x] Batch processing: 100 items/batch
  - [x] Return: {chunks_with_embeddings, images_with_embeddings, cost}
  - [x] Cost tracking: ~$0.0002 per document (embeddings only)
  
  **A2.5. run_pipeline.py - Stage 5: ChromaDB Indexing** ‚úÖ DONE (Jan 4)
  - [x] Replace print ‚Üí logging ‚úÖ
  - [x] Add function: `index_to_chromadb(doc_id, chunks, images)` ‚úÖ
  - [x] Support incremental adds (don't rebuild entire index) ‚úÖ
  - [x] Separate collections: text_chunks, image_captions ‚úÖ
  - [x] LangChain Chroma integration (compatible with retriever.py) ‚úÖ
  - [x] Metadata: PDF (page_num) + JSON (image_index) support ‚úÖ
  - [x] Deduplication: skip already indexed items ‚úÖ

- [x] **A3. Create unified orchestrator** ‚úÖ PARTIAL (Jan 3)
  
  **run_pipeline.py** 
  - [ ] Command: `download` (Stage 1) - NOT IMPLEMENTED
    - [ ] `--source arxiv/medium/wikipedia/docs/all`
    - [ ] `--doc-ids` or `--keywords` based on source
    - [ ] Returns list of downloaded doc_ids
  
  - [x] Command: `process` (Stage 2) ‚úÖ WORKING
    - [x] `--doc-id` (single document processing)
    - [x] `--no-vlm` (skip Vision LLM)
    - [x] Auto-detect document type (PDF vs JSON)
    - [x] Stage 1: Extract (PDF via PyMuPDF, JSON via URL download)
    - [x] Stage 2: Caption (enrich with context)
    - [x] Stage 3: Chunk (page-based for PDF, position-based for JSON)
    - [x] Stage 4: Embed (text-embedding-3-small, 1536 dims) ‚úÖ
    - [x] Stage 5: Index to ChromaDB (incremental, LangChain Chroma) ‚úÖ DONE
  
  - [x] Registry management: ‚úÖ WORKING
    - [x] `processed_docs.json` with status tracking
    - [x] Schema: {doc_id: {status, stages, stats, cost, timestamps}}
    - [x] Update after each stage completion

- [ ] **A4. Remove intermediate JSON saves**
  - [ ] Keep only:
    - [ ] `documents_metadata.json` (master list, Stage 1 output)
    - [ ] `images_metadata.json` (master list, Stage 1 output)
    - [ ] `processed_docs.json` (processing registry, Stage 2 output)
  - [ ] Remove after ChromaDB build:
    - [ ] `chunks_metadata.json` ‚Üí only in ChromaDB
    - [ ] `chunks_with_embeddings.json` ‚Üí only in ChromaDB
    - [ ] `images_with_embeddings.json` ‚Üí only in ChromaDB

### Acceptance Criteria:
- ‚úÖ No print statements in production code (except CLI usage examples)
- ‚úÖ Stage 1: `python run_pipeline.py download --source arxiv --doc-ids 1706.03762`
- ‚úÖ Stage 2: `python run_pipeline.py process --incremental`
- ‚úÖ Logs saved to `logs/pipeline.log` with timestamps
- ‚úÖ Incremental processing works (skip already indexed docs)
- ‚úÖ Registry tracks: source, status, timestamp, counts, cost

### Current Progress (Jan 4, 2026):
- ‚úÖ **A0:** Test data cleaned
- ‚úÖ **A1:** Download scripts refactored (54 documents: 35 arXiv + 9 RealPython + 10 Medium/TDS)
- ‚úÖ **A2.1-A2.5:** Processing pipeline stages 1-5 complete ‚úÖ **ALL DONE**
  - ‚úÖ Multi-source extraction (PDF via PyMuPDF, JSON via URL download)
  - ‚úÖ Context extraction (200 chars before/after images)
  - ‚úÖ Vision LLM improvements (8 categories, tested on 4 images)
  - ‚úÖ Chunking with PDF/JSON support (page-based + position-based linking)
  - ‚úÖ Embeddings with text-embedding-3-small (1536 dims, batch processing)
  - ‚úÖ ChromaDB indexing with incremental adds (LangChain Chroma)
  - ‚úÖ In-memory pipeline optimization (registry: 2 KB for 4 docs, 99.5% reduction)
  - ‚úÖ Tested on 3 documents with full pipeline: arxiv_1409_3215, medium_agents-plan-tasks, realpython_numpy-tutorial
- ‚úÖ **A3:** Unified orchestrator (run_pipeline.py) - **COMPLETE**
  - ‚úÖ All 5 stages working end-to-end
  - ‚úÖ Incremental processing (skip completed stages)
  - ‚úÖ Force reprocessing with --force flag
  - ‚úÖ --no-vlm flag (uses existing enrichments)
  - ‚úÖ Registry tracking with status, stats, costs
- üéØ **Next:** Phase C - Full pipeline execution on all 54 documents

**Testing Results (Jan 4):**

**Stage 1-3 Testing (Jan 3):**
- PDF (arxiv_1409_3215): 22 chunks, 27% with figure references, page-based linking working
- JSON (medium_agents-plan-tasks): 5 chunks, 80% with related images, position-based linking working  
- JSON (realpython_numpy-tutorial): 24 chunks, 100% with related images, excellent distribution
- Average chunk size: 1,500-1,700 chars ‚âà 430-485 tokens ‚úÖ Target achieved

**Full Pipeline Testing with VLM (Jan 4):**

| Document | Type | Images | Chunks | VLM Cost | Embed Cost | Total Cost | Time |
|----------|------|--------|--------|----------|------------|------------|------|
| arxiv_1409_3215 | PDF | 2 | 22 | $0.030 | $0.00023 | **$0.030** | 50s |
| realpython_numpy-tutorial | JSON | 8 | 24 | $0.120 | $0.00028 | **$0.120** | 106s |
| medium_agents-plan-tasks | JSON | 2 | 5 | $0.000* | $0.00004 | **$0.000** | 5s |

*Tested with --no-vlm flag (uses existing enriched_caption)

**Key Findings:**
- ‚úÖ All enrichments preserved through all stages (bug fixed)
- ‚úÖ Registry optimized: 2.01 KB for 4 docs (was 113 KB before optimization)
- ‚úÖ In-memory pipeline: no full_text duplication, no images_metadata duplication
- ‚úÖ Embedding cost negligible: ~$0.0003 per document
- ‚úÖ VLM cost: $0.015 per image (~$0.030-$0.120 per doc depending on image count)
- ‚úÖ RealPython has 100% chunk-image linking (24/24 chunks with related images)
- ‚úÖ PDF has selective linking (5/22 chunks with related images, 6/22 with figure references)

**ChromaDB Index (Stage 5 - Jan 4):**
- ‚úÖ **3 documents indexed successfully**
- ‚úÖ **51 text chunks** in text_chunks collection
- ‚úÖ **12 image captions** in image_captions collection
- ‚úÖ **Incremental indexing working:** skips already indexed items
- ‚úÖ **Metadata compatibility:** PDF (page_num) + JSON (image_index) both supported
- ‚úÖ **LangChain Chroma:** compatible with retriever.py
- ‚úÖ **Collections:** 
  - `data/chroma_db/text_chunks/` - text chunks with embeddings + metadata
  - `data/chroma_db/image_captions/` - image captions with embeddings + metadata

---

## Phase B: Dataset Expansion üìö

**Duration:** 2-3 days  
**Priority:** üî¥ HIGH  
**Status:** ‚è≥ PENDING

### B1. Medium/TDS Articles (5-7 articles)

**Target Articles:**
- "Understanding Convolutional Neural Networks"
- "A Gentle Introduction to LSTM"
- "Attention Mechanism Explained"
- "Transfer Learning in Computer Vision"
- "Batch Normalization: What, Why, How"

**Tasks:**
- [ ] Create `ingest/scrape_medium.py`
- [ ] Use BeautifulSoup for HTML parsing
- [ ] Extract: title, author, date, content, code blocks, images
- [ ] Save to `data/raw/medium_{article_id}/`
- [ ] Handle Medium paywall (use cached/free articles)

### B2. Wikipedia Articles (5-7 articles)

**Target Topics:**
- Convolutional neural network
- Recurrent neural network
- Transformer (machine learning model)
- Backpropagation
- Gradient descent
- Overfitting
- Activation function

**Tasks:**
- [ ] Create `ingest/download_wikipedia.py`
- [ ] Use `wikipedia` Python library
- [ ] Extract text sections + Wikimedia images
- [ ] Parse structured format (sections ‚Üí chunks)
- [ ] Save to `data/raw/wiki_{topic_id}/`

### B3. Official Documentation (5-7 pages)

**Target Docs:**
- PyTorch: nn.Module, torch.optim, DataLoader
- TensorFlow: Keras layers, tf.data
- Scikit-learn: Pipeline, GridSearchCV

**Tasks:**
- [ ] Create `ingest/scrape_docs.py`
- [ ] Download HTML from official sites
- [ ] Parse with BeautifulSoup
- [ ] Extract code blocks + diagrams
- [ ] Save to `data/raw/docs_{framework}_{page}/`

### B4. Additional arXiv Papers (5-7 papers)

**Target Papers:**
- BERT (1810.04805)
- GPT-2 (Language Models are Unsupervised Multitask Learners)
- YOLO (You Only Look Once)
- U-Net (Convolutional Networks for Biomedical Image Segmentation)
- GAN (Generative Adversarial Networks)

**Tasks:**
- [ ] Use existing `ingest/extract_images_smart.py`
- [ ] Download papers via arXiv API
- [ ] Process 5-7 papers in batch

### Acceptance Criteria:
- ‚úÖ 20-25 documents collected from all sources
- ‚úÖ Consistent folder structure: `data/raw/{doc_id}/`
- ‚úÖ All images extracted with metadata
- ‚úÖ `documents_metadata.json` updated with new docs

---

## Phase C: Full Pipeline Execution üîÑ

**Duration:** 1 day  
**Priority:** üü° MEDIUM  
**Status:** ‚è≥ PENDING

### Tasks:

- [ ] **C1. Clean test data**
  - [ ] Delete existing ChromaDB: `data/chroma_db/`
  - [ ] Clear processed JSONs: `data/processed/embeddings/`
  - [ ] Keep only raw documents: `data/raw/`

- [ ] **C2. Run unified pipeline**
  ```bash
  python run_pipeline.py --source all --incremental
  ```
  - [ ] Process all 20+ documents
  - [ ] Generate captions with OpenAI Vision
  - [ ] Chunk with new params (1800 chars, 200 overlap)
  - [ ] Embed with text-embedding-3-small
  - [ ] Build ChromaDB collections

- [ ] **C3. Verify ChromaDB statistics**
  - [ ] Text chunks: ~500-700 expected
  - [ ] Images: ~150-250 expected
  - [ ] Test retrieval on sample queries
  - [ ] Check metadata fields populated correctly

- [ ] **C4. Update PRD with final stats**
  - [ ] Document counts per source type
  - [ ] Average chunk size, image count
  - [ ] ChromaDB storage size

### Acceptance Criteria:
- ‚úÖ ChromaDB contains 500+ text chunks
- ‚úÖ ChromaDB contains 150+ images
- ‚úÖ Test query returns relevant results from multiple sources
- ‚úÖ No errors in pipeline logs

---

## Phase D: System Evaluation üìä

**Duration:** 1-2 days  
**Priority:** üü¢ REQUIRED  
**Status:** üîÑ IN PROGRESS

**Progress:**
- ‚úÖ Created eval/ directory structure
- ‚úÖ Test queries defined (30 queries: 10 text, 10 visual, 10 hybrid)
- ‚úÖ Retrieval testing on 3 indexed documents complete
- ‚úÖ Automated logging and metrics to eval/results/
- ‚è≥ Pending: Full evaluation on all 54 documents

### D1. Create Evaluation Dataset

**30 Test Queries:**

**Text-focused (10):**
1. "What is backpropagation?"
2. "Explain dropout regularization"
3. "How does batch normalization work?"
4. "What is the vanishing gradient problem?"
5. "Difference between CNN and RNN?"
6. "What is transfer learning?"
7. "Explain cross-entropy loss"
8. "How does Adam optimizer work?"
9. "What is overfitting?"
10. "Explain gradient descent"

**Visual (10):**
11. "Show ResNet architecture"
12. "Display LSTM cell diagram"
13. "Show Transformer model"
14. "Illustrate CNN layers"
15. "Show attention mechanism"
16. "Display GAN architecture"
17. "Show U-Net structure"
18. "Diagram of backpropagation"
19. "Show activation functions"
20. "Display neural network layers"

**Hybrid (10):**
21. "Explain residual connections and show skip connections"
22. "What is attention mechanism? Show formula"
23. "How does LSTM work? Show gates"
24. "Explain CNN architecture with diagram"
25. "What is multi-head attention? Show parallel heads"
26. "How does GAN train? Show discriminator/generator"
27. "Explain encoder-decoder and show architecture"
28. "What is batch normalization? Show computation graph"
29. "How does dropout work? Show visualization"
30. "Explain Adam optimizer with update rules"

**Tasks:**
- [x] Create `eval/test_queries.json` with 30 queries
- [x] Create `eval/test_retrieval_indexed.py` for testing on indexed docs
- [x] Setup eval/results/ directory for logs and metrics
- [ ] Manually label ground truth:
  - [ ] Relevant chunk_ids for each query
  - [ ] Relevant image_ids for visual/hybrid queries
- [ ] Save ground truth in `eval/ground_truth.json`

**Results (3 indexed documents):**
- ‚úÖ Image hit rate: 87.5% (7/8 queries retrieved images)
- ‚úÖ Avg images per query: 1.5
- ‚úÖ MEDIUM confidence: 8 images, LOW: 4 images
- ‚úÖ Visual queries: 100% hit rate
- ‚úÖ Text queries: 67% hit rate
- ‚úÖ Document filter prevents cross-document pollution
- üìÅ Results saved to: eval/results/retrieval_summary_YYYYMMDD_HHMMSS.json

### D2. Retrieval Metrics

**Metrics to Measure:**
- **Recall@5:** % of relevant chunks in top-5 (target ‚â•70%)
- **Image Hit Rate:** % of visual queries with ‚â•1 relevant image in top-5 (target ‚â•60%)
- **MRR (Mean Reciprocal Rank):** Average 1/rank of first relevant result

**Tasks:**
- [ ] Create `eval/evaluate_retrieval.py`
- [ ] Run retrieval on 30 test queries
- [ ] Compare results to ground truth
- [ ] Generate metrics report

### D3. Answer Quality Metrics

**Metrics to Measure:**
- **Faithfulness:** % of answers supported by retrieved sources (target ‚â•80%)
- **Citation Accuracy:** % of citations actually relevant (target ‚â•85%)
- **"I don't know" Correctness:** Does system refuse when context insufficient? (target 100%)

**Tasks:**
- [ ] Create `eval/evaluate_answers.py`
- [ ] Generate answers for 30 test queries
- [ ] Manual review:
  - [ ] Check each claim against sources
  - [ ] Verify citations point to relevant content
  - [ ] Test off-topic queries (system should say "I don't know")
- [ ] Generate quality report

### D4. Latency Profiling

**Tasks:**
- [ ] Add timing instrumentation to retriever.py
- [ ] Measure:
  - [ ] Text retrieval time (semantic search)
  - [ ] Image retrieval time (metadata + verification)
  - [ ] Total retrieval time
  - [ ] Generation time (reasoning tokens + answer)
  - [ ] End-to-end latency
- [ ] Generate performance report
- [ ] Compare medium vs low reasoning effort

### Acceptance Criteria:
- ‚úÖ Recall@5 ‚â• 70%
- ‚úÖ Image Hit Rate ‚â• 60%
- ‚úÖ Faithfulness ‚â• 80%
- ‚úÖ Citation Accuracy ‚â• 85%
- ‚úÖ End-to-end latency < 60 seconds (medium reasoning)
- ‚úÖ Evaluation report generated in `eval/results/`

---

## Phase E: Final Optimization ‚ö°

**Duration:** 1 day  
**Priority:** üü¢ OPTIONAL  
**Status:** ‚è≥ PENDING

### Tasks:

- [ ] **E1. Based on evaluation results:**
  - [ ] If Recall@5 < 70%: Consider increasing k_text (3‚Üí5)
  - [ ] If Image Hit Rate < 60%: Adjust similarity thresholds
  - [ ] If latency > 60s: Test reasoning_effort="low"
  - [ ] If chunks too large/small: Adjust chunk_size

- [ ] **E2. Documentation updates:**
  - [ ] Update PRD.md with final metrics
  - [ ] Update README.md:
    - [ ] Setup instructions
    - [ ] API keys configuration
    - [ ] How to run pipeline
    - [ ] How to run UI
  - [ ] Create docs/PROMPT_ENGINEERING.md:
    - [ ] Document 7 iterations of prompt refinement
    - [ ] Lessons learned
    - [ ] Final prompt structure

- [ ] **E3. Optional UI enhancements:**
  - [ ] Query history in sidebar
  - [ ] Export answers to PDF/markdown
  - [ ] Adjustable k_text slider
  - [ ] Sample query buttons
  - [ ] Show token counts and latency in debug view

### Acceptance Criteria:
- ‚úÖ All metrics meet targets
- ‚úÖ Documentation complete and accurate
- ‚úÖ README has step-by-step setup guide
- ‚úÖ Code ready for demo/presentation

---

## Timeline Summary

| Phase | Duration | Start | End | Status |
|-------|----------|-------|-----|--------|
| A. Code Cleanup | 1-2 days | Jan 2 | Jan 3-4 | ‚è≥ PENDING |
| B. Dataset Expansion | 2-3 days | Jan 4 | Jan 6-7 | ‚è≥ PENDING |
| C. Pipeline Execution | 1 day | Jan 7 | Jan 7-8 | ‚è≥ PENDING |
| D. System Evaluation | 1-2 days | Jan 8 | Jan 9-10 | ‚è≥ PENDING |
| E. Final Optimization | 1 day | Jan 10 | Jan 10-11 | ‚è≥ PENDING |
| **TOTAL** | **6-9 days** | **Jan 2** | **Jan 10-11** | |

---

## Technical Configuration

### Current Settings (Optimized):

```yaml
# Chunking
chunk_size: 1800  # ~500 tokens
chunk_overlap: 200  # ~55 tokens, 11% overlap

# Embeddings
model: "text-embedding-3-small"
dimensions: 1536

# Retrieval
k_text: 3  # Top-3 text chunks
k_images: 3  # Up to 3 verified images

# Generator
llm: "gpt-5-nano"  # OpenAI GPT-5 Nano
max_tokens: 15000  # Reasoning (8000) + Answer (4000)
reasoning_effort: "medium"  # Balance speed/quality
temperature: 0.1

# Verification
similarity_threshold: 0.6  # Same-page images
similarity_threshold_nearby: 0.7  # ¬±1 page images
visual_fallback_threshold: 0.5  # Fallback for visual queries
```

---

## Key Decisions Log

### January 2, 2026

**Decision 1: Chunk Size Reduction**
- Changed: 2800 chars (800 tokens) ‚Üí 1800 chars (500 tokens)
- Rationale: Better precision, less noise for LLM reasoning
- Expected impact: Retrieval precision ‚Üë, reasoning tokens ‚Üì

**Decision 2: Retrieval k=3**
- Changed: k=5 ‚Üí k=3
- Rationale: With smaller chunks, top-3 sufficient for single concept
- Expected impact: Faster retrieval, less context noise

**Decision 3: MAX_TOKENS=15000**
- Changed: 8000 ‚Üí 15000
- Rationale: Reasoning can use up to 8000, need buffer for answer
- Result: No more empty responses due to token exhaustion

**Decision 4: Reasoning Effort = Medium**
- Compared: low (128-2000 tokens) vs medium (1000-8000 tokens)
- Result: Medium provides concrete metrics (e.g., "ResNet-110 6.43% error")
- Trade-off: 60s latency acceptable for educational use case

**Decision 5: Embedding Dimensions = 1536**
- Considered: 1024 (33% storage savings)
- Decided: 1536 (full quality)
- Rationale: Technical content needs high precision, small corpus (storage not issue)

---

## Next Steps

**Immediate (Phase A):**
1. ‚úÖ Save this roadmap
2. üîÑ Delete test data (ChromaDB, processed JSONs)
3. üîÑ Audit code for cleanup opportunities
4. üîÑ Create `run_pipeline.py` skeleton

**This Week:**
- Complete Phase A (Code Cleanup)
- Start Phase B (Dataset Expansion)

**Next Week:**
- Complete Phase B, C, D
- Run full evaluation
- Finalize documentation

---

## Success Criteria

**Code Quality:**
- ‚úÖ No print statements in production code
- ‚úÖ Consistent logging across modules
- ‚úÖ Single command processes entire pipeline
- ‚úÖ Incremental indexing works correctly

**System Performance:**
- ‚úÖ Recall@5 ‚â• 70%
- ‚úÖ Image Hit Rate ‚â• 60%
- ‚úÖ Faithfulness ‚â• 80%
- ‚úÖ Citation Accuracy ‚â• 85%
- ‚úÖ Latency < 60 seconds

**Production Readiness:**
- ‚úÖ 20-25 documents indexed
- ‚úÖ 500+ text chunks, 150+ images
- ‚úÖ Comprehensive documentation
- ‚úÖ Evaluation metrics documented
- ‚úÖ Ready for demo/presentation
