# Roadmap: MVP â†’ Production System

**Project:** AI/ML Course Assistant - Multimodal RAG  
**Status:** Phase D.A2 Complete (95% Recall, 88.9% Image Hit Rate, MRR=1.0)  
**Last Updated:** January 9, 2026

---

## Overview

**Current State (Jan 9, 2026):**
- âœ… **Phase A Complete:** Code cleanup, refactoring, unified pipeline
- âœ… **54 documents curated:** 35 arXiv + 9 RealPython + 10 Medium/TDS
- âœ… **19 documents fully indexed:** 369 text chunks, 142 images with VLM descriptions
- âœ… **Phase D.A1 Complete:** Ground truth dataset (10 queries, validated 100%)
- âœ… **Phase D.A2 Complete:** Retrieval evaluation with 3/3 targets achieved
  - âœ… Recall@5: 95.0% (target â‰¥70%)
  - âœ… Image Hit Rate: 88.9% (target â‰¥60%)  
  - âœ… MRR: 1.000 (target â‰¥0.70)
- âœ… **Critical Bugs Fixed:**
  - âœ… ChromaDB path mismatch (retriever compatibility)
  - âœ… Missing image_id in metadata (+33.3% Image Hit Rate improvement)
  - âœ… JSON metadata deserialization (related_image_ids)
- âœ… **Evaluation Tools:** evaluate_retrieval.py, test_retrieval_indexed.py, validate_ground_truth.py
- âœ… **VLM Integration Complete:** gpt-4.1-mini descriptions (~$0.015/image)
- âœ… **Reasoning Optimization:** GPT-5 Nano "low" effort (-85% reasoning tokens)
- âœ… **Documentation:** PIPELINE_GUIDE (70+ KB), context_retrieval_evaluation_results.md

**Next Milestones:**
- ğŸ¯ **Jan 9:** Phase D.B1 - Faithfulness Judge with LLM (â‰¥4.0/5.0 target)
- ğŸ¯ **Jan 10:** Phase D - Final evaluation report with Top 3 improvements
- ğŸ¯ **Jan 11:** Production-ready demo
- ğŸ¯ **Jan 12:** Final presentation preparation

---

## Phase A: Code Cleanup & Refactoring ğŸ“

**Duration:** 1-2 days  
**Priority:** ğŸ”´ HIGH  
**Status:** âœ… **COMPLETE** (Jan 2-5, 2026)

### Architecture Decision: 2-Stage Pipeline

**Stage 1: Data Collection** ğŸ“¥
- User downloads raw documents from sources
- Manual curation/review before processing
- Output: `data/raw/{source}/{doc_id}/`

**Stage 2: Processing & Indexing** âš™ï¸
- Automated pipeline: Extract â†’ Caption â†’ Chunk â†’ Embed â†’ Index
- Incremental: Only new documents (check `processed_docs.json`)
- Output: ChromaDB collections

### Tasks:

- [x] **A0. Clean test data** âœ…
  - [x] Deleted `data/chroma_db/` (test ChromaDB)
  - [x] Deleted `data/processed/` (intermediate files)
  - [x] Deleted test scripts: clean_metadata.py, analyze_openai_results.py, test_extraction.py
  - [x] Ready for fresh start

- [x] **A1. Refactor download scripts (Stage 1)** âœ…
  
  **A1.1. download_arxiv.py** âœ… DONE
  - [x] Replace all `print()` â†’ `logging.info/debug/error`
  - [x] Keep only essential CLI in `__main__`
  - [x] Add function: `download_papers(paper_ids: List[str]) -> List[str]`
  - [x] Return downloaded doc_ids for tracking
  - [x] 35 curated papers with complete DL coverage
  
  **A1.2. download_realpython.py** âœ… DONE
  - [x] BeautifulSoup for HTML scraping
  - [x] Function: `download_articles(urls: List[str]) -> List[str]`
  - [x] Extract code blocks + images with captions
  - [x] Save: `data/raw/realpython/{slug}/{slug}.json`
  - [x] 9 articles downloaded (316 code blocks, 115 images)
  
  **A1.3. download_medium.py** âœ… DONE
  - [x] BeautifulSoup for HTML scraping
  - [x] Function: `download_articles(urls: List[str]) -> List[str]`
  - [x] Multi-selector fallback for TDS (article â†’ article-content â†’ post-content â†’ body)
  - [x] Image caption parsing with figcaption support
  - [x] Spam filtering (recommended stories, follow buttons)
  - [x] Save: `data/raw/medium/{slug}/{slug}.json`
  - [x] 10 articles downloaded (RAG evaluation, chunking, agents, metrics)
  
  **A1.4. download_docs.py** âŒ SKIPPED
  - Official docs not included in dataset (0 pages in PRD)

- [x] **A5. Context Extraction Algorithm Improvements** âœ… DONE (Jan 8)
  
  **Problem Analysis:**
  - [x] Analyzed 21 images across 4 documents (2 arXiv PDFs + 2 JSON sources)
  - [x] Identified 4 major issues:
    1. Fixed 200-char window cuts sentences mid-word
    2. Primitive position estimation (assumes 10 images max)
    3. No relevance checking (technical context for decorative images)
    4. Literal caption search fails for generic captions
  
  **Implementation (JSON sources - extract_from_json.py):**
  - [x] `_should_skip_context()`: Detects decorative images via VLM description
    - Patterns: "image by author", "not a technical", "kitten", "animal"
    - Result: Returns empty context for 4 RealPython kitty photos âœ…
  - [x] `_extract_sentence_boundary()`: Extracts full sentences instead of fixed chars
    - Searches for '. ', '!\n', '?\n' markers
    - Max 250 chars instead of 200
    - Result: No more mid-word cuts âœ…
  - [x] `_find_position_by_keywords()`: Smart position finding
    - Extracts key terms from caption (first/last 5 words)
    - Estimates by paragraphs instead of proportional division
    - Formula: `(image_index / (image_index + 5))` instead of `/10`
    - Result: Better position accuracy âœ…
  - [x] `find_context_for_image()`: Enhanced main function
    - Added `vlm_description` parameter
    - Integrates all 3 helper functions
    - Returns empty strings for irrelevant images
  
  **Implementation (PDF sources - extract_image_context.py):**
  - [x] `_extract_sentence_boundary_from_text()`: PDF-specific sentence extraction
    - Uses sentence boundaries instead of fixed 200â†’250 chars
    - Separate logic for before (from_end=True) and after (from_end=False)
  - [x] `_group_text_into_paragraphs()`: Groups text by y-coordinates
    - Detects paragraph breaks (y-gap > 5 pixels)
    - Returns (paragraph_text, start_y, end_y) tuples
    - Uses PDF coordinate system for precise grouping
  - [x] `_extract_context_from_previous_page()`: Cross-page fallback
    - Extracts last sentence from previous page if context_before empty
    - Handles Figure 1 at page start gracefully
  - [x] `extract_surrounding_context()`: Enhanced with bbox coordinates
    - Uses image bbox y-coordinates to filter before/after text
    - Groups text into paragraphs for better context
    - Cross-page fallback: doc + page_num parameters
    - Max 250 chars with sentence boundaries
  
  **Results (Old Algorithm):**
  - PDF images: 73% good context (8/11) - empty before, wrong Figure/Table
  - JSON images: 50% good context (4/8) - decorative photos misleading
  - Overall: 67% success rate (14/21)
  
  **Expected Results (New Algorithm):**
  - PDF images: 90%+ (sentence boundaries + cross-page fallback)
  - JSON images: 100% (skip logic filters decorative)
  - Overall: 85-90% success rate
  
  **Documentation:**
  - [x] docs/context_extraction_improvements.md: Full algorithm comparison
  - [x] docs/context_analysis_pdf_images.md: PDF-specific analysis (11 images)
  - [x] Examples: Medium technical âœ…, RealPython kitty photos filtered âœ…
  - [x] PDF improvements: sentence boundaries + cross-page + paragraph grouping âœ…

- [x] **A2. Refactor processing scripts (Stage 2)** âœ… DONE (Jan 3)
  
  **A2.1. extract_images_smart.py + extract_from_json.py** âœ… DONE
  - [x] PDF: extract_document(doc_id) returns Dict with images/text
  - [x] JSON: extract_json_document(doc_id) for RealPython/Medium
  - [x] Context extraction: 200 chars before/after images
  - [x] Dual strategy: caption search + position estimation
  
  **A2.2. enrich_images.py** âœ… DONE
  - [x] generate_captions_for_doc(doc_id, use_vlm) function
  - [x] Enhanced Vision LLM prompt (8 categories: academic + IDE + workflows)
  - [x] Web images support (uses stored context from JSON)
  - [x] Tested on 4 images with excellent results (avg 410 words)
  
  **A2.3. chunk_documents.py** âœ… DONE
  - [x] Replaced print â†’ logging
  - [x] chunk_document_with_image_tracking(doc_id, full_text, ...)
  - [x] PDF: page-based linking (page_num)
  - [x] JSON: position-based linking (image_index + relative position)
  - [x] Params: chunk_size=1800 (~500 tokens), overlap=200 (~60 tokens)
  - [x] Tested on 3 docs: 1 PDF (22 chunks) + 2 JSON (5+22 chunks)
  
  **A2.4. index/embedding_utils.py** âœ… DONE (Jan 4)
  - [x] Replace print â†’ logging âœ…
  - [x] Add function: `embed_document(doc_id: str) -> Dict` âœ…
  - [x] Model: text-embedding-3-small (1536 dims)
  - [x] Batch processing: 100 items/batch
  - [x] Return: {chunks_with_embeddings, images_with_embeddings, cost}
  - [x] Cost tracking: ~$0.0002 per document (embeddings only)
  
  **A2.5. run_pipeline.py - Stage 5: ChromaDB Indexing** âœ… DONE (Jan 4)
  - [x] Replace print â†’ logging âœ…
  - [x] Add function: `index_to_chromadb(doc_id, chunks, images)` âœ…
  - [x] Support incremental adds (don't rebuild entire index) âœ…
  - [x] Separate collections: text_chunks, image_captions âœ…
  - [x] LangChain Chroma integration (compatible with retriever.py) âœ…
  - [x] Metadata: PDF (page_num) + JSON (image_index) support âœ…
  - [x] Deduplication: skip already indexed items âœ…

- [x] **A3. Create unified orchestrator** âœ… PARTIAL (Jan 3)
  
  **run_pipeline.py** 
  - [ ] Command: `download` (Stage 1) - NOT IMPLEMENTED
    - [ ] `--source arxiv/medium/wikipedia/docs/all`
    - [ ] `--doc-ids` or `--keywords` based on source
    - [ ] Returns list of downloaded doc_ids
  
  - [x] Command: `process` (Stage 2) âœ… WORKING
    - [x] `--doc-id` (single document processing)
    - [x] `--no-vlm` (skip Vision LLM)
    - [x] Auto-detect document type (PDF vs JSON)
    - [x] Stage 1: Extract (PDF via PyMuPDF, JSON via URL download)
    - [x] Stage 2: Caption (enrich with context)
    - [x] Stage 3: Chunk (page-based for PDF, position-based for JSON)
    - [x] Stage 4: Embed (text-embedding-3-small, 1536 dims) âœ…
    - [x] Stage 5: Index to ChromaDB (incremental, LangChain Chroma) âœ… DONE
  
  - [x] Registry management: âœ… WORKING
    - [x] `processed_docs.json` with status tracking
    - [x] Schema: {doc_id: {status, stages, stats, cost, timestamps}}
    - [x] Update after each stage completion

- [ ] **A4. Remove intermediate JSON saves**
  - [ ] Keep only:
    - [ ] `documents_metadata.json` (master list, Stage 1 output)
    - [ ] `images_metadata.json` (master list, Stage 1 output)
    - [ ] `processed_docs.json` (processing registry, Stage 2 output)
  - [ ] Remove after ChromaDB build:
    - [ ] `chunks_metadata.json` â†’ only in ChromaDB
    - [ ] `chunks_with_embeddings.json` â†’ only in ChromaDB
    - [ ] `images_with_embeddings.json` â†’ only in ChromaDB

### Acceptance Criteria:
- âœ… No print statements in production code (except CLI usage examples)
- âœ… Stage 1: `python run_pipeline.py download --source arxiv --doc-ids 1706.03762`
- âœ… Stage 2: `python run_pipeline.py process --incremental`
- âœ… Logs saved to `logs/pipeline.log` with timestamps
- âœ… Incremental processing works (skip already indexed docs)
- âœ… Registry tracks: source, status, timestamp, counts, cost

### Final Progress (Jan 5-8, 2026):
- âœ… **A0-A3:** All tasks complete
- âœ… **A5:** Context extraction algorithm improved (Jan 8)
  - âœ… Smart boundary detection (sentences instead of fixed chars)
  - âœ… Decorative image filtering (skip logic for non-technical images)
  - âœ… Improved position finding (keyword extraction + paragraph estimation)
  - âœ… 67% â†’ 85-90% expected success rate
  - âœ… Documentation: context_extraction_improvements.md, context_analysis_pdf_images.md
- âœ… **MMR Retrieval Enhancement:**
  - âœ… Implemented MMR (Maximal Marginal Relevance) for text chunk diversity
  - âœ… Text retrieval: MMR (Î»=0.7) for sequential coherence
  - âœ… Image retrieval: Similarity search with document filtering
  - âœ… Tested: Sequential chunks (4â†’5â†’6 vs 4â†’5â†’10) âœ… Improved
  - âœ… Image hit rate: 87.5% (7/8 test queries)
  - âœ… Document filter prevents cross-document pollution
- âœ… **Documentation Complete:**
  - âœ… PIPELINE_GUIDE.md (70+ KB comprehensive guide)
  - âœ… retrieval_strategy_analysis.md (MMR vs similarity comparison)
  - âœ… context_extraction_improvements.md (algorithm improvements)
  - âœ… context_analysis_pdf_images.md (PDF context analysis)
  - âœ… README.md updated with current status
  - âœ… All docs pushed to GitHub
- âœ… **Evaluation Framework (Phase D):**
  - âœ… eval/ground_truth.json - 10 annotated queries (5 text, 3 visual, 2 hybrid)
  - âœ… eval/validate_ground_truth.py - 100% validation success
  - âœ… eval/evaluate_retrieval.py - Full evaluation system (347 lines)
    - Metrics: Recall@k (k=3,5,10), Precision@k, MRR, Image Hit Rate
    - Auto-saves results to eval/results/retrieval_eval_<timestamp>.json
    - Console summary with target comparison
  - âœ… eval/test_retrieval_indexed.py - Quick testing tool
  - âœ… Phase D.A2 Results: Recall@5=95%, Image Hit Rate=88.9%, MRR=1.0 (3/3 targets âœ…)
- ğŸ¯ **Next:** Phase D.B1 - Faithfulness Judge with LLM (rag/generator.py evaluation)

**Testing Results (Jan 4):**

**Stage 1-3 Testing (Jan 3):**
- PDF (arxiv_1409_3215): 22 chunks, 27% with figure references, page-based linking working
- JSON (medium_agents-plan-tasks): 5 chunks, 80% with related images, position-based linking working  
- JSON (realpython_numpy-tutorial): 24 chunks, 100% with related images, excellent distribution
- Average chunk size: 1,500-1,700 chars â‰ˆ 430-485 tokens âœ… Target achieved

**Full Pipeline Testing with VLM (Jan 4 â†’ Jan 7):**

| Document | Type | Images | Chunks | VLM Cost | Embed Cost | Total Cost | Time |
|----------|------|--------|--------|----------|------------|------------|------|
| arxiv_1409_3215 | PDF | 2 | 22 | $0.030 | $0.00023 | **$0.030** | 50s |
| realpython_numpy-tutorial | JSON | 8 | 24 | $0.120 | $0.00028 | **$0.120** | 106s |
| medium_agents-plan-tasks | JSON | 2 | 5 | $0.030 | $0.00004 | **$0.030** | 42s |

**Jan 7 VLM Regeneration:**
- âœ… **ChromaDB completely cleaned** (forced fresh indexing)
- âœ… **All 3 documents reprocessed WITH VLM**
- âœ… **12 images with rich VLM descriptions** (1500-2500 chars each)
- âœ… **Total cost: $0.18** (VLM) + $0.0006 (embeddings) = **$0.181**
- âœ… **gpt-4.1-mini used:** Technical image descriptions (VS Code UI, plots, notebooks)

**Key Findings:**
- âœ… All enrichments preserved through all stages (bug fixed)
- âœ… Registry optimized: 2.01 KB for 3 docs (was 113 KB before optimization)
- âœ… In-memory pipeline: no full_text duplication, no images_metadata duplication
- âœ… Embedding cost negligible: ~$0.0003 per document
- âœ… VLM cost: **$0.015 per image** (~$0.030-$0.120 per doc depending on image count)
- âœ… RealPython has 100% chunk-image linking (24/24 chunks with related images)
- âœ… PDF has selective linking (5/22 chunks with related images, 6/22 with figure references)
- âœ… **VLM Impact:** Queries retrieve correct images with detailed context (92% faithfulness)

**ChromaDB Index (Stage 5 - Jan 7 with VLM):**
- âœ… **3 documents indexed successfully**
- âœ… **51 text chunks** in text_chunks collection (5+24+22)
- âœ… **12 image captions** in image_captions collection (2+8+2) **WITH VLM**
- âœ… **Incremental indexing working:** skips already indexed items
- âœ… **Metadata compatibility:** PDF (page_num + _vector_ pattern) + JSON (image_index + _web_ pattern)
- âœ… **LangChain Chroma:** compatible with retriever.py
- âœ… **Collections:** 
  - `data/chroma_db/text_chunks/` - text chunks with embeddings + metadata
  - `data/chroma_db/image_captions/` - **image captions with VLM descriptions** + embeddings + metadata

**UI Improvements (Jan 7):**
- âœ… **Image Path Bug Fixed:** Added `_vector_` pattern support (PDF vector graphics)
- âœ… **Patterns supported:** `_embedded_` (PDF rasters), `_vector_` (PDF vectors), `_web_` (JSON sources)
- âœ… **Streamlit UI working:** All images display correctly

**Generator Optimization (Jan 7):**
- âœ… **GPT-5 Nano reasoning optimization:** "medium" â†’ "low" effort
- âœ… **Token reduction:** ~85% reasoning tokens saved (7500 â†’ 256-1280)
- âœ… **MAX_TOKENS:** 15000 â†’ 10000 (sufficient for retrieval answers)
- âœ… **No quality loss:** Answers remain detailed and accurate

---

## Phase B: Dataset Expansion ğŸ“š

**Duration:** 2-3 days  
**Priority:** ğŸ”´ HIGH  
**Status:** â³ PENDING

### B1. Medium/TDS Articles (5-7 articles)

**Target Articles:**
- "Understanding Convolutional Neural Networks"
- "A Gentle Introduction to LSTM"
- "Attention Mechanism Explained"
- "Transfer Learning in Computer Vision"
- "Batch Normalization: What, Why, How"

**Tasks:**
- [ ] Create `ingest/scrape_medium.py`
- [ ] Use BeautifulSoup for HTML parsing
- [ ] Extract: title, author, date, content, code blocks, images
- [ ] Save to `data/raw/medium_{article_id}/`
- [ ] Handle Medium paywall (use cached/free articles)

### B2. Wikipedia Articles (5-7 articles)

**Target Topics:**
- Convolutional neural network
- Recurrent neural network
- Transformer (machine learning model)
- Backpropagation
- Gradient descent
- Overfitting
- Activation function

**Tasks:**
- [ ] Create `ingest/download_wikipedia.py`
- [ ] Use `wikipedia` Python library
- [ ] Extract text sections + Wikimedia images
- [ ] Parse structured format (sections â†’ chunks)
- [ ] Save to `data/raw/wiki_{topic_id}/`

### B3. Official Documentation (5-7 pages)

**Target Docs:**
- PyTorch: nn.Module, torch.optim, DataLoader
- TensorFlow: Keras layers, tf.data
- Scikit-learn: Pipeline, GridSearchCV

**Tasks:**
- [ ] Create `ingest/scrape_docs.py`
- [ ] Download HTML from official sites
- [ ] Parse with BeautifulSoup
- [ ] Extract code blocks + diagrams
- [ ] Save to `data/raw/docs_{framework}_{page}/`

### B4. Additional arXiv Papers (5-7 papers)

**Target Papers:**
- BERT (1810.04805)
- GPT-2 (Language Models are Unsupervised Multitask Learners)
- YOLO (You Only Look Once)
- U-Net (Convolutional Networks for Biomedical Image Segmentation)
- GAN (Generative Adversarial Networks)

**Tasks:**
- [ ] Use existing `ingest/extract_images_smart.py`
- [ ] Download papers via arXiv API
- [ ] Process 5-7 papers in batch

### Acceptance Criteria:
- âœ… 20-25 documents collected from all sources
- âœ… Consistent folder structure: `data/raw/{doc_id}/`
- âœ… All images extracted with metadata
- âœ… `documents_metadata.json` updated with new docs

---

## Phase C: Full Pipeline Execution ğŸ”„

**Duration:** 1 day  
**Priority:** ğŸ”´ HIGH  
**Status:** â³ **READY TO START** (Jan 7, 2026)

### C1. Full Pipeline Run âš¡

**Current Status:**
- âœ… 3 documents indexed WITH VLM (arxiv_1409_3215, medium_agents-plan-tasks, realpython_numpy-tutorial)
- âœ… Pilot evaluation complete (92% faithfulness, 80% image hit rate)
- â³ 51 documents remaining (35 arXiv PDFs + 16 JSON articles)
- â³ **Expected cost:** ~$0.75-1.00 (VLM for ~50-60 more images)

**Tasks:**

- [ ] **Pre-flight checks:**
  - [ ] Verify all 54 documents in `data/raw_data/`
  - [ ] Check OpenAI API quota (embeddings ~$0.01 for 51 docs)
  - [ ] Backup current ChromaDB (3 docs indexed)
  - [ ] Review registry: `python run_pipeline.py status`

- [ ] **Batch processing strategy:**
  - [ ] **Option A:** All at once WITH VLM (20-30 min, ~$0.75-1.00)
    ```bash
    python run_pipeline.py process --all
    ```
  - [ ] **Option B:** By source type (safer, easier to debug)
    ```bash
    # arXiv papers first (35 docs, ~30-50 images)
    python run_pipeline.py process --source arxiv
    
    # Then RealPython (9 docs, remaining processed with VLM)
    python run_pipeline.py process --source realpython
    
    # Finally Medium/TDS (10 docs)
    python run_pipeline.py process --source medium
    ```
  - [ ] **Recommendation:** Use Option A (full VLM) based on pilot success (92% faithfulness)

- [ ] **Monitor execution:**
  - [ ] Watch logs for errors
  - [ ] Track registry updates: `processed_docs.json`
  - [ ] Verify ChromaDB growth: text_chunks â†’ ~500-700, image_captions â†’ ~60-80 (WITH VLM)

- [ ] **Post-processing validation:**
  - [ ] Check final counts: `python run_pipeline.py status --summary`
  - [ ] Verify all 54 documents completed
  - [ ] Test retrieval on sample queries
  - [ ] Document any failures

**Expected Results:**
- âœ… ~500-700 text chunks indexed
- âœ… ~60-80 image captions indexed (WITH VLM descriptions)
- âœ… Total cost: **~$0.90-1.20** (VLM ~$0.75 + embeddings ~$0.015)
- âœ… Processing time: 20-30 minutes (with VLM 3sec delays)
- âœ… No errors in pipeline logs

### Acceptance Criteria:
- âœ… All 54 documents in registry with status="completed"
- âœ… ChromaDB contains 500+ text chunks
- âœ… ChromaDB contains 60+ image captions **WITH VLM descriptions**
- âœ… Test queries return results from multiple sources
- âœ… No processing errors
- âœ… All images have rich VLM descriptions (1500-2500 chars)

---

## Phase D: System Evaluation ğŸ“Š

 
**Priority:*ğŸ”„ **IN PROGRESS** 
**Progress:**
- âœ… eval/ directory structure created
- âœ… **Pilot evaluation complete:** 5 queries on 3 documents
- âœ… **Faithfulness: 92%** (4.6/5 average) â¬…ï¸ **EXCELLENT**
- âœ… **Citation correctness: 80%** (1 bug identified)
- âœ… **Image hit rate: 80%** (4/5 queries)
- âœ… **Reasoning optimization validated:** -85% tokens with NO quality loss
- âœ… **VLM impact confirmed:** Answers reference detailed UI/code/plot context
- âœ… Documentation: `eval/results/pilot_3docs.md`
- â³ Pending: Full evaluation on all 54 documents (20-50 queries)

### D0. Pilot Evaluation Results (Jan 7) âœ… COMPLETE

**Summary Table:**

| Query | Type | Images | Faithfulness | Citations | Reasoning Tokens |
|-------|------|--------|-------------|-----------|-----------------|
| 1. AI agents planning | Visual | 2 | **5/5** âœ… | âœ… | 256 |
| 2. JupyterLab examples | Visual | 2 | **5/5** âœ… | âœ… | 384 |
| 3. LSTM architecture | Technical | 2 | **4/5** âš ï¸ | âœ… | 576 |
| 4. NumPy indexing | Text | 0 | **5/5** âœ… | âœ… | 704 |
| 5. Lorenz visualization | Hybrid | 2 | **4/5** âš ï¸ | âŒ | 1280 |
| **Average** | â€” | 1.6 | **4.6/5** (92%) | 80% | **622** |

**Key Findings:**
- âœ… **VLM Critical for Quality:** Queries with images had 92% faithfulness vs potentially lower without VLM context
- âœ… **Reasoning "low" Perfect:** 622 avg tokens vs ~7500 with "medium" (**85% reduction, NO quality loss**)
- âœ… **Image Verification Works:** Query 4 correctly rejected 5 irrelevant images (sim < 0.50)
- âŒ **Citation Bug:** Query 5 phantom [C] reference needs investigation (likely generator issue)
- âš ï¸ **Fallback Retrieval:** Query 3 LOW confidence (missing chunkâ†’image metadata links)

**VLM Impact Evidence:**
- Query 2 & 5: **0 text chunks cited** - answers built ENTIRELY from VLM descriptions
- Answers reference: "VS Code UI", "right panel", "terminal output", "7 to-dos checklist", "Lorenz.ipynb file"
- Technical details: Code line numbers, syntax highlighting colors, specific Python commands
- **Cost: $0.015/image** (~$0.18 for 12 images) - **High ROI for quality**s/
- â³ Pending: Full evaluation on all 54 documents

### D1. Ground Truth Creation (Jan 7-8)

**Manual Labeling Required:**

- [ ] **For each of 30 test queries:**
  - [ ] Manually search ChromaDB for relevant chunks
  - [ ] Label 3-5 relevant chunk_ids per query
  - [ ] Label 1-3 relevant image_ids for visual/hybrid queries
  - [ ] Rate relevance: HIGHLY_RELEVANT, SOMEWHAT_RELEVANT, NOT_RELEVANT

- [ ] **Save ground truth:**
  ```json
  {
    "query_001": {
      "query": "What is backpropagation?",
      "relevant_chunks": ["arxiv_1502_03167_chunk_0005", ...],
      "relevant_images": [],
      "notes": "Focus on algorithm explanation, not history"
    }
  }
  ```
  - [ ] File: `eval/ground_truth.json`

- [ ] **Quality checks:**
  - [ ] Each query has â‰¥3 relevant chunks labeled
  - [ ] Visual queries have â‰¥1 relevant image
  - [ ] No duplicates or typos in IDs


### D2. Retrieval Metrics Evaluation (Jan 8)

**Metrics Implementation:**

- [ ] **Create `eval/evaluate_retrieval.py`:**
  ```python
  def calculate_recall_at_k(ground_truth, retrieved_chunks, k=5):
      # % of relevant chunks in top-k
  
  def calculate_mrr(ground_truth, retrieved_chunks):
      # Mean reciprocal rank of first relevant
  
  def calculate_image_hit_rate(ground_truth, retrieved_images):
      # % of visual queries with â‰¥1 relevant image
  ```

- [ ] **Run evaluation:**
  ```bash
  python eval/evaluate_retrieval.py --queries eval/test_queries.json --ground-truth eval/ground_truth.json
  ```

- [ ] **Generate report:**
  - [ ] Recall@5 per query type (text/visual/hybrid)
  - [ ] MRR distribution
  - [ ] Image hit rate breakdown
  - [ ] Failure analysis (queries with Recall@5 < 0.5)

**Target Metrics:**
- âœ… Recall@5 â‰¥ 70%
- âœ… Image Hit Rate â‰¥ 60%
- âœ… MRR â‰¥ 0.5

### D3. UI Testing & Improvements 

**Streamlit UI Tasks:**

- [ ] **Test current UI:**
  - [ ] Launch: `streamlit run ui/app.py`
  - [ ] Test 10 sample queries
  - [ ] Verify image rendering
  - [ ] Check text chunk display
  - [ ] Test query history

- [ ] **Improvements needed:**
  - [ ] Add confidence badges (HIGH/MEDIUM/LOW) for images
  - [ ] Show similarity scores
  - [ ] Display source metadata (doc_id, page_num)
  - [ ] Add "Copy citation" button
  - [ ] Implement query suggestions
  - [ ] Add latency metrics display

- [ ] **Generator integration (if time permits):**
  - [ ] Connect retriever to LLM (gpt-4o-mini)
  - [ ] Add answer generation view
  - [ ] Display citations inline
  - [ ] Implement "I don't know" logic

### D4. Answer Quality Evaluation (Jan 8-9)

**Prerequisites:**
- â³ Generator implementation required

**Tasks:**

- [ ] **Create `eval/evaluate_answers.py`:**
  ```python
  def calculate_faithfulness(answer, retrieved_chunks):
      # Check if claims supported by sources
  
  def calculate_citation_accuracy(answer, citations, ground_truth):
      # Verify citations point to relevant content
  
  def test_idk_correctness(off_topic_queries):
      # System should refuse unanswerable queries
  ```

- [ ] **Manual review (30 answers):**
  - [ ] Read each generated answer
  - [ ] Verify claims against retrieved sources
  - [ ] Check citation accuracy
  - [ ] Rate quality 1-5

- [ ] **Off-topic query testing:**
  - [ ] Create 10 off-topic queries (e.g., "How to cook pasta?")
  - [ ] Verify system responds "I don't know" or similar
  - [ ] Test edge cases (partially related queries)

**Target Metrics:**
- âœ… Faithfulness â‰¥ 80%
- âœ… Citation Accuracy â‰¥ 85%
- âœ… "I don't know" Correctness = 100%

### Acceptance Criteria:
- âœ… Ground truth created for 30 queries
- âœ… Recall@5 â‰¥ 70%
- âœ… Image Hit Rate â‰¥ 60%
- âœ… UI tested and improved
- âœ… Generator integrated (if applicable)
- âœ… Evaluation reports generated in `eval/results/`

---

## Phase E: Final Optimization âš¡

**Duration:** 1 day  
**Priority:** ğŸŸ¢ OPTIONAL  
**Status:** â³ PENDING

### Tasks:

- [ ] **E1. Based on evaluation results:**
  - [ ] If Recall@5 < 70%: Consider increasing k_text (3â†’5)
  - [ ] If Image Hit Rate < 60%: Adjust similarity thresholds
  - [ ] If latency > 60s: Test reasoning_effort="low"
  - [ ] If chunks too large/small: Adjust chunk_size

- [ ] **E2. Documentation updates:**
  - [ ] Update PRD.md with final metrics
  - [ ] Update README.md:
    - [ ] Setup instructions
    - [ ] API keys configuration
    - [ ] How to run pipeline
    - [ ] How to run UI
  - [ ] Create docs/PROMPT_ENGINEERING.md:
    - [ ] Document 7 iterations of prompt refinement
    - [ ] Lessons learned
    - [ ] Final prompt structure

- [ ] **E3. Optional UI enhancements:**
  - [ ] Query history in sidebar
  - [ ] Export answers to PDF/markdown
  - [ ] Adjustable k_text slider
  - [ ] Sample query buttons
  - [ ] Show token counts and latency in debug view

### Acceptance Criteria:
- âœ… All metrics meet targets
- âœ… Documentation complete and accurate
- âœ… README has step-by-step setup guide
- âœ… Code ready for demo/presentation

---

## Timeline Summary

| Phase | Duration | Start | End | Status |
|-------|----------|-------|-----|--------|
| A. Code Cleanup & MMR | 3 days | Jan 2 | Jan 5 | âœ… COMPLETE |
| **Break Day** | **1 day** | **Jan 6** | **Jan 6** | **ğŸ–ï¸ OFF** |
| C. Full Pipeline Run | 1 day | Jan 7 | Jan 7 | â³ SCHEDULED |
| D1-D2. Ground Truth + Metrics | 2 days | Jan 7 | Jan 8 | â³ NEXT |
| D3. UI Testing | 1 day | Jan 8 | Jan 8 | â³ NEXT |
| D4. Answer Quality | 1 day | Jan 9 | Jan 9 | â³ NEXT |
| E. Refinements | 1-2 days | Jan 10 | Jan 11 | â³ PENDING |
| **TOTAL** | **9-11 days** | **Jan 2** | **Jan 11-13** | |

**Key Milestones:**
- âœ… Jan 5: MMR retrieval complete, GitHub push
- ğŸ–ï¸ **Jan 6: Break day (other work)**
- ğŸ¯ Jan 7: All 54 documents indexed
- ğŸ¯ Jan 8: Ground truth + UI testing complete
- ğŸ¯ Jan 9: Full evaluation metrics
- ğŸ¯ Jan 11: Production-ready demo

---

## Technical Configuration

### Current Settings (Optimized Jan 5):

```yaml
# Chunking
chunk_size: 1800  # ~500 tokens
chunk_overlap: 200  # ~55 tokens, 11% overlap

# Embeddings
model: "text-embedding-3-small"
dimensions: 1536
batch_size: 100

# Retrieval
search_type: "mmr"  # Maximal Marginal Relevance
mmr_lambda: 0.7  # 70% relevance, 30% diversity
k_text: 3  # Top-3 diverse text chunks
k_images: 3  # Up to 3 verified images

# Image Retrieval
image_search_type: "similarity"  # Not MMR (small dataset)
similarity_threshold: 0.5  # Same-page images
similarity_threshold_nearby: 0.65  # Â±1 page images
visual_fallback_threshold: 0.5  # Fallback for visual queries
document_filter: true  # Prevent cross-document pollution

# Generator (pending implementation)
llm: "gpt-4o-mini"  # OpenAI GPT-4o-mini
max_tokens: 4096
temperature: 0.1
```

**Key Improvements (Jan 5):**
- âœ… MMR for text chunks (sequential coherence 4â†’5â†’6)
- âœ… Document-filtered fallback (no cross-document images)
- âœ… Lower similarity thresholds (0.5 for better recall)
- âœ… Batch embedding processing (100 items/batch)

---

## Key Decisions Log

### January 2, 2026

**Decision 1: Chunk Size Reduction**
- Changed: 2800 chars (800 tokens) â†’ 1800 chars (500 tokens)
- Rationale: Better precision, less noise for LLM reasoning
- Expected impact: Retrieval precision â†‘, reasoning tokens â†“

**Decision 2: Retrieval k=3**
- Changed: k=5 â†’ k=3
- Rationale: With smaller chunks, top-3 sufficient for single concept
- Expected impact: Faster retrieval, less context noise

**Decision 3: MAX_TOKENS=15000**
- Changed: 8000 â†’ 15000
- Rationale: Reasoning can use up to 8000, need buffer for answer
- Result: No more empty responses due to token exhaustion

**Decision 4: Reasoning Effort = Medium**
- Compared: low (128-2000 tokens) vs medium (1000-8000 tokens)
- Result: Medium provides concrete metrics (e.g., "ResNet-110 6.43% error")
- Trade-off: 60s latency acceptable for educational use case

**Decision 5: Embedding Dimensions = 1536**
- Considered: 1024 (33% storage savings)
- Decided: 1536 (full quality)
- Rationale: Technical content needs high precision, small corpus (storage not issue)

---7, 2026)

**Morning (Phase C):**
1. âœ… Verify OpenAI API quota
2. ğŸ”„ Run full pipeline: `python run_pipeline.py process --all --no-vlm`
3. ğŸ”„ Monitor execution (4-25 min)
4. ğŸ”„ Validate results: 500+ chunks, 150+ images

**Afternoon (Phase D1):**
1. ğŸ”„ Start ground truth creation (30 queries)
2. ğŸ”„ Label 3-5 relevant chunks per query
3. ğŸ”„ Label images for visual/hybrid queries
4. ğŸ”„ Save to `eval/ground_truth.json`

**Evening (Phase D3):**
1. ğŸ”„ Test Streamlit UI with full dataset
2. ğŸ”„ Try 10-15 sample queries
3. ğŸ”„ Note UI improvements needed
4. ğŸ”„ Plan generator integration

**This Week Focus:**
- Jan 7: Full dataset indexed (54 docs)
- Jan 8: Ground truth + UI testing
- Jan 9: Evaluation metrics + answer quality
- Jan 10-11: Refinements and demo prepries)
- UI functional and tested
- Evaluation metrics calculated

---

## Technical Configuration (Updated Jan 5, 2026)

**Code Quality:**
- âœ… No print statements in production code
- âœ… Consistent logging across modules
- âœ… Single command processes entire pipeline
- âœ… Incremental indexing works correctly

**System Performance:**
- âœ… Recall@5 â‰¥ 70%
- âœ… Image Hit Rate â‰¥ 60%
- âœ… Faithfulness â‰¥ 80%
- âœ… Citation Accuracy â‰¥ 85%
- âœ… Latency < 60 seconds

**Production Readiness:**
- âœ… 20-25 documents indexed
- âœ… 500+ text chunks, 150+ images
- âœ… Comprehensive documentation
- âœ… Evaluation metrics documented
- âœ… Ready for demo/presentation
